- OWASP의 LLM 취약점
	- Prompt Injection : 조작된 입력으로 LLM 조작
	- Insecure Output Handling : LLM에서 생성된 출력이 다른 구성요소와 시스템으로 다운스트림으로 전달되기 전에, 유효성 검사, 정리 및 처리가 충분하지 않은것을 구체적으로 나타냄.
	- Training Data Poisoning : 사전 학습 데이터 또는 미세 조정에 관련된 데이터를 조작
	- Model Denial of Service :매우 많은 양의 리소스를 소비하는 방법으로 LLM과 상호작용 -> 서비스 품질 저하, 잠재적으로 높은 리소스 비용 발생
	- Supply Chain Vulnerabilities : 교육 데이터, ML 모델 및 배포 플랫폼의 무결성에 영향을 미침, 편향된 결과, 보안 침해 또는 시스템 장애
	- Sensitive Information Disclosure : 결과물을 통해 민감한 정보, 독점 알고리즘 또는 기타 기밀 세부 정보를 공해
	- Insecure Plugin Design : 상호작용 중에 모델에서 자동으로 호출되는 확장 프그램. 데이터 유출, 원격 코드 실행, 권한 상승 등 해로운 결과 초래
	- Excessive Agency : 다른 시스템과 상호 작용하고 프롬프트에 따라 작업을 수행 할 수 있는 권한. 과도한 행위는 LLM의 예상치 못한/모호한 출력에 대한 응답으로 유해한 행동을 수행
	- Overreliance : 과도한 의존. 사실과 다르거나 부적절한 콘텐츠 생성
	- Model Theft (모델 도난) : 악의적 행위자 또는 APT에 의한 LLM 모델에 대한 무단 액세스 및 유출

- OWASP의 LLM AI Security Checklist
	- Adversarial Risk(적대적 위험) 
		- 경쟁자나 공격자가 LLM을 악용함으로써 발생할 수 있는 위험 지적
	- Threat Modeling(위협 모델링)
		- LLM을 도입, 운영하는 과정에서 발생할 수 있는 잠재적 위협을 체계적으로 식별하고 평가하는 방법론
	- AI Asset Inventory (AI 자산관리)
		- 회사 안 또는 외부에서 생성된 조직 내의 모든 AI 관련 자산을 체계적으로 관리
		- LLM을 포함한 AI 모델이나, 데이터 소스, 툴, 소유자와 상태를 종합적으로 관리하는 프로세스 구축
	- AI Security and Privacy Training (AI보안, 프라이버시 교육)
		- 모든 직원이 LLM과 관련된 보안 위험과 프라이버시 문제를 이해하고, 이에 대한 대응 방법 숙지
		- AI 기술의 기본원리, 보안 인식, 개인정보 보호법 준수, LLM의 윤리적 사용에 관한 교육 필요
	- Establish Business Cases (비즈니스 목표와 계획)
		- LLM 도입의 타당성을 평가하고, 비즈니스 목표를 달성하기 위한 계획을 세우는 과정
		- LLM이 제공하는 경제적 가치, 예상 비용, 리스크, 투자 수익률 평가 등이 포함
			- 고객 경험 향상
			- 운영 효율성 향상
			- 더 나은 지식 관리
			- 향상된 혁신
			- 시장조사 및 경쟁사 분석
			- 문서작성, 번역, 요약 및 분석
		- Governance (거버넌스)
			- LLM 사용과 관련된 모든 활동을 관리, 감독하는 체계적인 절차 구축, 데이터 관리, AI 시스템의 책임 및 권한 분배, 정책 수립, 법적 및 규제적 요구사항 준수
		- Leagl (법적 고려 사항)
			- LLM 의 사용과 관련된 법적 위험을 식별하고, 이에 대한 대응 방안 마련
			- 제품 보증, 이용야관, 지식재산권 보호, 개인정보 보호 규정 준수, 책임 소재 명확화 등이 포함
		- Regulatory (규제)
			- LLM 사용과 관련된 모든 법적 및 규제적 요구사항을 충족하는 것을 의미
			- 국가나 지역별로 다른 AI 관려 법률과 규정을 준수해야 하며, 데이터 보호법, 개인정보 보호법, 그리고 AI 시스템의 투명성 및 윤리적 사용을 보장하는 규제를 포함
		- Using or Implementing LLM Solution (LLM 솔루션 사용과 구현)
			- LLM 조직의 인프라에 통합하고 운영하는 과정 전반에 관한 것.
			- LLM 배포, 데이터 보안, 접근 제어, 모델 유지 보수, 그리고 성능 최적화 등을 포함
		- Testing, Evaluation, Verification, and Validation (LLM 테스트, 평가, 검증, 확인)
			- LLM 기능성과 보안성을 검증하기 위한 전반적인 테스트 및 평가 과정
			- 모델의 성능 평가, 보안 취약점 테스트, 데이터 무결성 검증, 결과의 일관성 평가
		- Using or Implementing LLM Solution (RAG)
			- LLM 도입 후 성능을 개선하고 특정 도메인이나 사용 사례에 맞게 모델을 튜닝하는 과정
			- 모델의 정확도를 높이고, 응답 시간을 단축하며, 리소스 사용을 효율화하는 등의 과정을 포함
		- Using or Implementing LLM Solution (AI 레드팀 훈련)
			- LLM의 보안성을 평가하기 위해 공격 시나리오를 시뮬레이션하고, 잠재적인 보안 취약점을 탐지
			- 실제로 공격자가 시스템을 어떻게 공격할 수 있는지를 파악하고 보안 방어 체계를 강화하도록 하고, LLM 보안성을 지속해서 평가하고 개선 필요
	- LLM 보안 요구 사항
		- 학습 데이터 검증 및 필터링
			- 학습 데이터 자체에 민감한 정보나 편향성이 들어갈 경우, 모델 출력에 직접 반영
			- 학습에 쓰이는 데이터를 자세히 검토
			- 부적절한 내용은 사전에 제거
			- 특히 외부에 공개된 데이터나 크롤링 결과를 사용할 때 더욱 주의
		- 모델 출력 결과 모니터링 및 개인정보 노출 방지
			- 생성한 텍스트에 악의적이거나 해로운 내용이 없는지 실시간으로 모니터링
			- 개인정보보호법 등 개인정보 관련 규정도 준수
			- 서명, 주민등록번호 등 개인 식별 정보나, 다른 정보와 결합하면 특정 개인을 알아볼수 있는 정보를 노출하는지 여부를 확인
			- 특정 키워드나 패턴을 탐지하고 경고를 보내고해당 부분을 마스킹 처리하는 방법 등 활용
		- 웹 보안
			- 생성 결과에 대한 충분한 검사나 보안 처리
			- 웹 환경에서 주로 쓰이는 공격을 간접적인 방법으로 수행해봄 (XSS, CSRF, SSRF 등)
		- 공급망 보안
			- 한국 인터넷 진흥원에서는 SW 공급망 보안 가이드라인을 마련하여 배포함 (https://www.kisa.or.kr/skin/doc.html?fn=20240912_103321_876.pdf&rs=/result/2024-09/)
		- 제로 트러스트 및 모니터링
			- 제로 트러스트 : 어떤 사용자 또는 소프트웨어도 기본적으로 신뢰할 수 없다는 가정
			- 사용자 프롬프트와 외부 콘텐츠, LLM과 외부 플러그인 사이 등 모든 연동에 신뢰 관계 설정
			- 항상 최소 권한만 부여하며, 의도치 않게 서로 영향을 끼치지 않도록 항상 검증
			- 프롬프트를 확인하고, 필요시 안전하게 변환하며, LLM의 응답 결과를 다시 검증하는 일련의 과정 필요
		- 보안 자동화 및 교육
			- 개발 초기 단계부터 종합적인 보안을 고려해서 개발
			- 보안 위협으로 인해 발생하는 비용을 줄이고 서비스의 전체적인 보안 수준을 증가
			- 주기적으로 모델을 업데이트하고 보안 점검 실시

# Adversarial Attacks On LLMs
## 위협 모델
- 적대적 공격: 모델이 원치 않는 것을 출력하도록 트리거 하는 입력
- LLM의 맥락에서 공격은 추론 시간에만 발생하고, 모델의 가중치가 고정되어 있다고 가정
- Classification, Text Generation,  White-box vs Black-box
### 공격 패턴
- 회피 공격 : 거위 앞에 마이크 사진을 넣어서 거위로 분류하지 못하게하고, speech로 분류 하도록 유도
- 중독 공격 : AI 시스템 자체를 손상시키는 공격
- 탐색적 공격 : 모델 전도 공격, API를 통한 추출 공격
### 적대적 공격의 기능
- 언제 공격이 가능한지
	- 학습단계
		- 모델 학습에 사용되는 데이터셋을 조정하여 AI 모델에 직접 영향을 주거나 손상
	- 테스트 단계 기능
		- 테스트 단계에서 수행되는 공격은 AI 모델을 변경하지 않고 잘못된 출력을 야기함
		- 공격자가 모델에 대한 정보를 어느정도 보유하고 있는지에 따라 화이트박스 공격 또는 블랙박스 공격으로 구분
			- 화이트박스 공격
				- 공격자가 모델에 대한 모든 정보를 알고 공격하는 것
			- 블랙박스 공격
				- 공격자가 모델에 대한 정보를 알지 못하고 하는 공격
# Adversarial Prompting
LLM 기반으로 어플리케이션 생성 시 관련된 오용, 위험 및 안전에 대한 고민 필요
- 프롬프팅 엔지니어링에서 중요한 주제로 인식
- 이러한 모위험을 식별하고 문제를 해결하기 위한 기법 설계가 매우 중요 -> 모델의 기본원칙을 위배하고 우회 시킴
	- 적대적 프롬프팅
		- 프롬프트 주입, 프롬프트유출, 탈옥(범법행위, 비윤리적 지시)
	- 사실성
	- 편향
# Defence
- Defense-GAN
	- 적대적 생성 신경망(GAN) 알고리즘을 이용하여 적대적 공격 방어
	- 적대적 예제(Adversary Example)를 추가 학습 데이터로 활용하는 아이디어로 시작하여 변조된 이미지가 정상적인 이미지로 판단되도록 하는 것이 최종 목표
- 적대적 훈련
	- 가능한 모든 적대적 사례를 학습 데이터에 포함해 머신러닝을 훈련시키는 방법
	- 머신러닝을 훈련시키는 단계에서 예상 가능한 해킹된 데이터를 충분히 입력해 머신러닝의 저항성을 기르는 방식
- 결과값 분석 차단
	- 학습 모델의 결과값 분석을 통해 모델을 추론하는 방식의 공격을 차단하기 위해, ㅁ학습 모델의 결과값이 노출되지 않도록 하거나, 결과값을 분석할 수 없게 변환하는 방식으로 공격 차단
- 적대적 공격 여부 탐지
	- 원래의 모델과 별도로 적대적 공격여부를 판단하기 위한 모델을 추가한 후, 두 모델의 추론 결과를 비교해 두 결과 같에 큰 차이가 발생하는 경우 적대적 공격으로 탐지하는 방식
- 쿼리 횟수 제한
	- 모델에 반복적인 쿼리를 시도하는 Inversion attack이나 Model extraction attac를 방어하기 위해서 모델에 쿼리 횟수를 제한하는 방식
	- 학습 데이터에 포함된 기밀 정보, 민감정보가 노출되지 않도록 암호화 등의 비식별 처리 방식도 연구