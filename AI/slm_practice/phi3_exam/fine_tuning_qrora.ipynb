{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi3 Fine Tuning QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qqq --upgrade bitsandbytes transformers peft accelerate datasets trl flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install absl-py nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Parameter\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "dataset_name = \"wonik-hi/korea_summary_Thesis\"\n",
    "\n",
    "dataset_split= \"train\"\n",
    "\n",
    "new_model = \"phi-3-mini-QLoRA\"\n",
    "\n",
    "hf_model_repo=\"wonik-hi/\"+new_model\n",
    "\n",
    "# Load Model on GPU\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "se_4bit = True\n",
    "\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "use_double_quant = True\n",
    "\n",
    "# LoRA configuration for the model\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 불러오기\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "dataset = dataset.select(range(500)) #실제 훈련 시, 삭제\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 준비를 위한 Tokenizer 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.padding_side = 'right' # warnings 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗 메시지 \n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    user = {\n",
    "        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['output']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    messages.append(assistant)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chatml = dataset.map(create_message_column)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "dataset_chatml[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA and trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 인식 - 학습을 위해서는 GPU 이용\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'sdpa'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning을 위한 Tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n",
    "          attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"./phi-3-mini-QLoRA\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        #num_train_epochs=3,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import wandb\n",
    "os.envrion[\"PROJECT\"] = \"phi-3-mini-QLoRA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"phi-3-mini-QLoRA\"\n",
    "\n",
    "wandb.init(project=project_name, name = \"phi-3-mini-QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml['train'],\n",
    "        eval_dataset=dataset_chatml['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_adapter_repo=\"wonik-hi/phi-3-mini-QLoRA-adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(hf_adapter_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_adapter_repo = \"wonik-hi/phi-3-mini-QLoRA\"\n",
    "\n",
    "model_name, hf_adapter_repo, compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = hf_adapter_repo\n",
    "tr_model_id = model_name\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_id = hf_model_repo\n",
    "model.push_to_hub(merged_model_id)\n",
    "tokenizer.push_to_hub(merged_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_repo='wonik-hi/phi-3-mini-QLoRA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map, compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "set_seed(1234)  # For reproducibility\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map) # compute \"auto\" dev_map \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the dataset\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][0]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95, max_time= 180)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '%%time' is a magic command in Jupyter Notebook that measures the execution time of the cell it is placed in.\n",
    "%%time\n",
    "\n",
    "test_inference(dataset_chatml['test'][0]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metricas = dataset_chatml['test'].select(range(0,25)).map(calculate_rogue, batched=False)\n",
    "# 실제 수행 시, 하기 주석 풀고 상기 내용 삭제\n",
    "#metricas = dataset_chatml['test'].select(range(0,500)).map(calculate_rogue, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rouge 1 Mean: \",np.mean(metricas['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(metricas['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(metricas['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(metricas['rougeLsum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chatml['test'][0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples=500\n",
    "num_samples=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# '%%time' is a magic command in Jupyter Notebook that measures the execution time of the cell it is placed in.\n",
    "prompts = [pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][i]['messages'][0]['content']}],\n",
    "                                              tokenize=False, add_generation_prompt=True)\n",
    "                                              for i in range(num_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(prompts, batch_size=4, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180)\n",
    "preds = [outputs[i][0]['generated_text'].split(\"<|assistant|>\\n\")[1].strip() for i in range(len(outputs))]\n",
    "references= [dataset_chatml['test'][i]['output'] for i in range(len(outputs))]\n",
    "rouge_metric.add_batch(predictions=preds, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, 'result = rouge_metric.compute(use_stemmer=True)' calculates the Rouge scores with stemming and stores the result in the 'result' variable.\n",
    "result = rouge_metric.compute(use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rouge 1 Mean: \",np.mean(result['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(result['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(result['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(result['rougeLsum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROUGE-1 is a metric for evaluating automatic summarization of texts and machine translation. It compares the overlap of unigrams (single words) between the system's output and the reference summaries.\n",
    "result['rouge1']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
