{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Vertext AI 이용 멀티모달\n",
    "- 유료 모델임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI SDK\n",
    "!pip install google-cloud-apiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-cloud-apiplatform 설치 이후 주피터 재부팅 필요\n",
    "import IPython\n",
    "import time\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutodwn(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"geminitest-xxxx\"\n",
    "LOCATION = \"asia-northeast3\"\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini 10.0 Pro Vision model\n",
    "\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper  함수 \n",
    "\n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가격이미지와 사진 이미지로 가격정보 매핑\n",
    "\n",
    "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
    "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
    "image_grocery = load_image_from_url(image_grocery_url)\n",
    "image_prices = load_image_from_url(image_prices_url)\n",
    "\n",
    "instructions = \"Instructions: Consider the following image that contains fruits:\"\n",
    "prompt1 = \"How much should I pay for the fruits given the following price list?\"\n",
    "prompt2 = \"\"\"\n",
    "Answer the question through these steps:\n",
    "Step 1: Identify what kind of fruits there are in the first image.\n",
    "Step 2: Count the quantity of each fruit.\n",
    "Step 3: For each grocery in first image, check the price of the grocery in the price list.\n",
    "Step 4: Calculate the subtotal price for each type of fruit.\n",
    "Step 5: Calculate the total price of fruits using the subtotals.\n",
    "\n",
    "Answer and describe the steps taken:\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    instructions,\n",
    "    image_grocery,\n",
    "    prompt1,\n",
    "    image_prices,\n",
    "    prompt2,\n",
    "]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용방법 알려주기\n",
    "\n",
    "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
    "image_stove = load_image_from_url(image_stove_url)\n",
    "\n",
    "prompt = \"\"\"How can I reset the clock on this appliance?\n",
    "Provide the instructions in English and French.\n",
    "If instructions include buttons, also explain where those buttons are physically located.\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "contents = [image_stove, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다이어그램에서 엔티티 관계 이해하기\n",
    "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
    "image_er = load_image_from_url(image_er_url)\n",
    "\n",
    "prompt = \"Document the entities and relationships in this ER diagram.\"\n",
    "\n",
    "contents = [prompt, image_er]\n",
    "\n",
    "# Use a more deterministic configuration with a low temperature\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 이미지에서 추천하기 - 얼굴 형태에 맞는 안경 추천\n",
    "\n",
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Which of these glasses you recommend for me based on the shape of my face?\n",
    "I have an oval shape face.\n",
    "----\n",
    "Glasses 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "----\n",
    "Glasses 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "----\n",
    "Explain how you reach out to this decision.\n",
    "Provide your recommendation based on my face shape, and reasoning for each in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사성/차이점\n",
    "\n",
    "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
    "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
    "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
    "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Consider the following two images:\n",
    "Image 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Image 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "1. What is shown in Image 1?\n",
    "2. What is similar between the two images?\n",
    "3. What is difference between Image 1 and Image 2 in terms of the contents or people shown?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a video description\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is shown in this video?\n",
    "Where should I go to see it?\n",
    "What are the top 5 places in the world that look like this?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting tags of obejct throughout the video\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "- What is in the video?\n",
    "- What is the action in the video?\n",
    "- Provide 10 best tags for this video?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking more questions about a video\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving extra information beyond the video\n",
    "\n",
    "prompt = \"\"\"\n",
    "Which line is this?\n",
    "where does it go?\n",
    "What are the stations/stops?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
