{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit_chat pix2tex transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from pix2tex.cli import LatexOCR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
    "from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"wonik-hi/phi3_fine_tuning\"\n",
    "math_model = LatexOCR()\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import torch\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from pix2tex.cli import LatexOCR\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
    "from streamlit_chat import message\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "@st.cache_resource()\n",
    "def load_model():\n",
    "    #model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    model_id=\"oz1115/phi3_fine_tuning\"\n",
    "    math_model = LatexOCR()\n",
    "\n",
    "    torch.random.manual_seed(0)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    )\n",
    "    #\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer =streamer,\n",
    "    )\n",
    "    return math_model, model\n",
    "\n",
    "def image_math(image):\n",
    "    latex_code = math_model(image)\n",
    "    if '\\stackrel' in latex_code:\n",
    "        latex_code = latex_code.split('\\stackrel')[0]\n",
    "    elif latex_code.startswith('\\left['):\n",
    "        latex_code = latex_code.split('\\left[')[1]\n",
    "    else:\n",
    "        pass\n",
    "    return latex_code\n",
    "\n",
    "def generate_response(user_input, uploaded_file):\n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file)\n",
    "        latex_code = image_math(image)\n",
    "        new_image = image.resize((500, 150))\n",
    "        st.image(new_image, caption=f\" OCR Result:  {latex_code} \")\n",
    "        contents = f\"{user_input}: {latex_code}\"\n",
    "    else:\n",
    "        contents = f\"{user_input}?\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": contents},]\n",
    "    return model(messages, **generation_args)[0]['generated_text']\n",
    "\n",
    "\n",
    "# model\n",
    "math_model, model = load_model()\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.5,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# st format setting\n",
    "st.header(\"ðŸ¤–Phi-3 for math (ì˜ì–´ë²„ì „)\")\n",
    "st.sidebar.markdown(\"## Information\")\n",
    "st.sidebar.info (\"- ìˆ˜í•™ë¬¸ì œ í’€ì´ë¥¼ ìœ„í•œ 'Phi-3'.\\n - í˜„) í•œê¸€ë¡œ ì§ˆë¬¸ì‹œ ë‹µë³€ì´ ì–´ë ¤ì›€.\\n - This tool uses 'Phi-3' to solve math problems.\\n - It is only understand 'English'\")\n",
    "st.sidebar.markdown(\"### Guide\")\n",
    "st.sidebar.write(\"1. (Optional) ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ / Upload a image file.\")\n",
    "st.sidebar.write(\"2. ì˜ì–´ë¡œ ì§ˆë¬¸í•˜ê¸° / Ask something.\\n\\t - ex) solve the problem.\")\n",
    "st.sidebar.write(\"3. ë²„íŠ¼ í´ë¦­ / Click 'Send' button to activate the AI.\")\n",
    "\n",
    "# do chat\n",
    "if 'generated' not in st.session_state:\n",
    "    st.session_state['generated'] = []\n",
    "\n",
    "if 'past' not in st.session_state:\n",
    "    st.session_state['past'] = []\n",
    "\n",
    "with st.form('form', clear_on_submit=True):\n",
    "    uploaded_file = st.file_uploader(\"Upload a file\", type=['png', 'jpg'], )\n",
    "    user_input = st.text_input('You: ', '', key='input')\n",
    "    submitted = st.form_submit_button('Send')\n",
    "\n",
    "if submitted and user_input :\n",
    "    with st.spinner('Thinking...'):\n",
    "        output = generate_response(user_input, uploaded_file)\n",
    "        st.session_state.past.append(user_input)\n",
    "        st.session_state.generated.append(output)\n",
    "\n",
    "if st.session_state['generated']:\n",
    "    st.button(\"Clear History\", on_click=lambda: st.session_state.clear())\n",
    "    for i in range(len(st.session_state['generated']) - 1, -1, -1):\n",
    "        message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')\n",
    "        message(st.session_state[\"generated\"][i], key=str(i))\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run /content/app.py &>/content/logs.txt &\n",
    "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
