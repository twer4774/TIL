{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Phi3\n",
    "- Phi3 기초 이해\n",
    "- 모델 파인 튜닝\n",
    "- 모델 평가\n",
    "- 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi3란\n",
    "마이크로소프트에서 개발된 모델. 같은 사이즈의 다른 모델들보다 추론, 코딩, 수학 벤치 마크 점수가 높다.\n",
    "\n",
    "- Phi-3-mini : 3.8B\n",
    "- Phi-3-small : 7B\n",
    "- Phi-3-medium : 14B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 1. Phi3 기반 파이썬 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"Your are a python developer\"},\n",
    "    {\"role\" : \"user\", \"content\": \"Help me generate a bubble algorithm\"}\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "     model=model,\n",
    "     tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 600,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.3,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 2. Phi3 Ollama 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggigface-hub>=0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi3 gguf파일 다운로드\n",
    "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir /content --local-dir-use-systemlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    "\n",
    "# 터미널 내에 입력\n",
    "\"\"\"\n",
    "curl -fsSL https://ollaama.com/install.sh | sh\n",
    "ollama create phi3 -f Modelfile_q4\n",
    "ollama serv & ollama pull phi3\n",
    "ollama run phi3 'your prompt here'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-core\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"phi3\")\n",
    "llm.invoke(\"Tell em 3 red flower names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers sentence_transformers faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 수학데이터 셋 약 2만5천개\n",
    "dataset = load_dataset(\"garage-bAInd/Open-Platypus\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wonik-hi/phi3_fine_tuning\")\n",
    "\n",
    "# 2. Tokenize each row and count the number of tokens\n",
    "instruction_token_counts = [len(tokenizer.tokenize(example[\"instruction\"])) for example in dataset['train']]\n",
    "output_token_counts = [len(tokenizer.tokenize(example[\"output\"])) for example in dataset['train']]\n",
    "combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]\n",
    "\n",
    "# Helper function to plot the distributions\n",
    "def plot_distribution(token_counts, title):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.hist(token_counts, bins=50, color='#3498db', edgecolor='black')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Number of tokens\", fontsize=14)\n",
    "    plt.ylabel(\"Number of examples\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the distribution of token counts\n",
    "plot_distribution(instruction_token_counts, \"Distribution of token counts for instruction only\")\n",
    "plot_distribution(output_token_counts, \"Distribution of token counts for output only\")\n",
    "plot_distribution(combined_token_counts, \"Distribution of token counts for combined instruction + output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 필터링1 - 전체 아웃풋의 토큰 크기가 2048개 이하인 경우 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with more than 2048 tokens\n",
    "valid_indices = [i for i, count in enumerate(combined_token_counts) if count <= 2048]\n",
    "print(f\"Number of valid rows: {len(valid_indices)}\")\n",
    "print(f\"Removing {len(dataset['train']) - len(valid_indices)} rows...\")\n",
    "\n",
    "\"\"\"\n",
    "#실제 훈련 시 주석 풀고 아래 줄 삭제\n",
    "# Extract valid rows based on indices\n",
    "#dataset['train'] = dataset['train'].select(valid_indices)\n",
    "\n",
    "# Get token counts for valid rows\n",
    "token_counts = [combined_token_counts[i] for i in valid_indices]\n",
    "\n",
    "plot_distribution(token_counts, \"New distribution of token counts for combined instruction + output\")\n",
    "\"\"\"\n",
    "\n",
    "# 실제 훈련 시 아래 내용 삭제\n",
    "dataset['train'] = dataset['train'].select(valid_indices[:10])   #--> 실제 훈련 시 삭제\n",
    "token_counts = [combined_token_counts[i] for i in valid_indices[:10]]\n",
    "plot_distribution(token_counts, \"New distribution of token counts for combined instruction + output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 필터링 2 - 중복되는 임베딩 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def deduplicate_dataset(dataset: Dataset, model: str, threshold: float):\n",
    "    ## loading embeddings\n",
    "    sentence_model = SentenceTransformer(model)\n",
    "    ## loading output datasets\n",
    "    outputs = [example[\"output\"] for example in dataset['train']]\n",
    "\n",
    "    print(\"Converting text to embeddings...\")\n",
    "    ## convert output to embeddings and normalize\n",
    "    embeddings = sentence_model.encode(outputs, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    index.add(normalized_embeddings)\n",
    "\n",
    "    print(\"Filtering out near-duplicates...\")\n",
    "    D, I = index.search(normalized_embeddings, k=2)\n",
    "    to_keep = []\n",
    "\n",
    "    for i in tqdm(range(len(embeddings)), desc=\"Filtering\"):\n",
    "        # If the second closest vector (D[i, 1]) has cosine similarity above the threshold\n",
    "        if D[i, 1] >= threshold:\n",
    "            # Check if either the current item or its nearest neighbor is already in the to_keep list\n",
    "            nearest_neighbor = I[i, 1]\n",
    "            if i not in to_keep and nearest_neighbor not in to_keep:\n",
    "                # If not, add the current item to the list\n",
    "                to_keep.append(i)\n",
    "        else:\n",
    "            # If the similarity is below the threshold, always keep the current item\n",
    "            to_keep.append(i)\n",
    "\n",
    "    dataset = dataset['train'].select(to_keep)\n",
    "    return DatasetDict({\"train\": dataset})\n",
    "\n",
    "deduped_dataset = deduplicate_dataset(dataset, \"thenlper/gte-large\", 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in the original dataset: {len(dataset['train'])}\")\n",
    "print(f\"Number of samples in the deduped dataset: {len(deduped_dataset['train'])}\")\n",
    "print(f\"Number of samples that were removed: {len(dataset['train']) - len(deduped_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Top k 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_rows(dataset, token_counts, k):\n",
    "    # Sort by descending token count and get top k indices\n",
    "    sorted_indices = sorted(range(len(token_counts)), key=lambda i: token_counts[i], reverse=True)\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    # Extract top k rows\n",
    "    top_k_data = {\n",
    "        \"instruction\": [dataset['train'][i][\"instruction\"] for i in top_k_indices],\n",
    "        \"output\": [dataset['train'][i][\"output\"] for i in top_k_indices]\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(top_k_data)\n",
    "\n",
    "# Get token counts\n",
    "instruction_token_counts = [len(tokenizer.tokenize(example[\"instruction\"])) for example in deduped_dataset['train']]\n",
    "output_token_counts = [len(tokenizer.tokenize(example[\"output\"])) for example in deduped_dataset['train']]\n",
    "combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]\n",
    "\n",
    "k = 1000  # You can adjust this value as needed\n",
    "top_k_dataset = get_top_k_rows(deduped_dataset, combined_token_counts, k)\n",
    "\n",
    "# Save these rows in a Dataset object with a 'train' split\n",
    "dataset = DatasetDict({\"train\": top_k_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_template(example):\n",
    "    example[\"instruction\"] = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K\n",
    "- 수학 데이터 셋 \n",
    "- 데이터셋을 결합하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gsm8k_m = load_dataset(\"openai/gsm8k\", 'main')\n",
    "dataset_gsm8k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", 'socratic')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_gsm8k = pd.DataFrame(dataset_gsm8k_m['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 합치기 위해 컬럼명 변경\n",
    "df_gsm8k = df_gsm8k.rename(columns={\"question\":\"instruction\", \"answer\":\"output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df, df_gsm8k])\n",
    "df_all"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
