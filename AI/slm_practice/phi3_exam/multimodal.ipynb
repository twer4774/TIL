{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal 정의\n",
    "- AI에서 modality는 \"데이터형식\"을 의미\n",
    "- 멀티모달 AI는 여러 개의 데이터 형식을 가지고 수행하는 AI\n",
    "    - input 양식 : 이미지 + 텍스트, 이미지 + 정형\n",
    "\n",
    "## Mutlmodal 예시\n",
    "### 음성인식\n",
    "- 음성으로 입력된 정보를 인식하여 텍스트로 변환하는 기술\n",
    "### 영상인식\n",
    "- 영상에서 물체나 사람을 인식하고, 그에 대한 정보를 제공하는 기술\n",
    "    - Google Lens : 카메라로 물체나 장면을 촬영하면 해당 물체나 자면에 대한 정보를 제공하는 서비스\n",
    "    - Face ID : 얼굴 인식 기술을 사용하여 iPhon 잠금을 해제하는 기술\n",
    "    - Selfie stick : 스마트폰을 부착하여 손을 자유롭게 사용할 수 있도록 만든 장치\n",
    "### 자연어 처리\n",
    "- 텍스트를 분석하여 의미를 파악하고, 그에 대한 응답을 생성하는 기술\n",
    "    - 구글 번역 : 텍스트나 음성을 번역하는 서비스\n",
    "    - 챗봇 : 사람과 대화하는 것처럼 사용자의 질문이나 요청에 응답하는 서비스\n",
    "    - AI 작가 : 텍스트를 생성하는     \n",
    "### 감정 인식\n",
    "- 사람의 얼굴이나 표정이나 목소리에 감정을 인식하는 기술\n",
    "    - 페이스북 감정 인식 : 사용자의 얼굴 표정에서 감정을 인식하여, 그에 맞는 광고를 노출하는 기술\n",
    "    - 감정 인식 AI : 사용자의 감정을 인식하여, 그에 맞는 대응을 하는 AI\n",
    "    - 감정 인식 로봇 : 사용자의 감정을 인시갛여, 그에 맞는 반응을 하는 로봇\n",
    "### AR/VR\n",
    "- 현실 세계에 가상의 정보를 결합하여 새로운 경험을 제공하는 기술\n",
    "    - Pocketmon GO : AR 이용\n",
    "    - 마인크래프트 : VR 이용\n",
    "    - 메타버스 : 현실 세계와 가상 세계를 결한한 새로운 가상 세계\n",
    "\n",
    "\n",
    "## Multimodal 종류\n",
    "- Early Fusion : 종류가 다른 두가지 데이터를 하나의 데이터로 먼저 합친 이후 모델을 학습\n",
    "- Late fusion : 종류가 다른 두가지 데이터를 각각 다른 모델에 학습 시킨 이후 나온 결과를 유향 (기존 앙상블 모델과 비슷한 방식)\n",
    "- Joint or Intermediate Fusion : 두개의 모달리티 데이터를 동시에 학습하지 않고, 내가 원하는 모델의 깊이에서 모달리티를 병합할 수 있는 유연성 포함\n",
    "\n",
    "\n",
    "## 사례 LLaVA\n",
    "- 오픈소스 챗봇\n",
    "- 이미지 해석과 대화 능력 금증\n",
    "- 시각적 입력을 소화하고 지시 능력을 확인하기 위한 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLavA 실습 - 실패함.\n",
    "- 코랩에서도 RAM 소진으로 실행 못해봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers==4.37.2\n",
    "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip uninstall timm -y\n",
    "!pip install timm==0.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit qauntization\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model using pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n",
    "\n",
    "# 양자화 안한 버전\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"image-to-text\", model=\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 200\n",
    "prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n",
    "\n",
    "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlaVa 실습 2 - 직접 설치로 하는 방법 - 실패\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone -b dev https://github.com/camenduru/LLaVA\n",
    "%cd /content/LLaVA\n",
    "\n",
    "!pip install -q transformers==4.36.2\n",
    "!pip install ninja\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "!pip install -e .\n",
    "\n",
    "# !python -m llava.serve.cli \\\n",
    "#     --model-path liuhaotian/llava-v1.5-7b \\\n",
    "#     --image-file \"https://llava-vl.github.io/static/images/view.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 모드\n",
    "# 모델을 다운로드 받아서 사용\n",
    "!pip install huggingface_hub protobuf\n",
    "%cd /content\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"liuhaotian/llava-v1.5-7b\", local_dir=\"llava-v1.5-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "threading.Thread(target=lambda: subprocess.run(['python3', '-m', 'llava.serve.controller', '--host', '0.0.0.0', '--port', '10000'], check=True), daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "# Define the command to run the model worker\n",
    "command = [\n",
    "    'python3', '-m', 'llava.serve.model_worker',\n",
    "    '--host', '0.0.0.0',\n",
    "    '--controller', 'http://localhost:10000',\n",
    "    '--port', '40000',\n",
    "    '--worker', 'http://localhost:40000',\n",
    "    '--model-path', '/content/llava-v1.5-7b',\n",
    "    '--load-8bit'\n",
    "]\n",
    "\n",
    "# Run the command in a separate thread\n",
    "thread = threading.Thread(\n",
    "    target=lambda: subprocess.run(command, check=True, shell=False),\n",
    "    daemon=True\n",
    ")\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --share"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
