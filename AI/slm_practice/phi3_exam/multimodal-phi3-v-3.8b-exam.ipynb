{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3-V-3.8B-Inference-Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/mbzuai-oryx/LLaVA-pp.git\n",
    "%cd LLaVA-pp\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cp Phi-3-V/train.py LLaVA/llava/train/train.py\n",
    "! cp Phi-3-V/llava_phi3.py LLaVA/llava/model/language_model/llava_phi3.py\n",
    "! cp Phi-3-V/builder.py LLaVA/llava/model/builder.py\n",
    "! cp Phi-3-V/model__init__.py LLaVA/llava/model/__init__.py\n",
    "! cp Phi-3-V/main__init__.py LLaVA/llava/__init__.py\n",
    "! cp Phi-3-V/conversation.py LLaVA/llava/conversation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd LLaVA\n",
    "! pip install --upgrade pip\n",
    "! pip install -e .\n",
    "! pip install git+https://github.com/huggingface/transformers@a98c41798cf6ed99e1ff17e3792d6e06a2ff2ff3\n",
    "\n",
    "! export PYTHONPATH=\"./:$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! git lfs install\n",
    "! git clone https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.utils import disable_torch_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_PLACEHOLDER\n",
    "from llava.conversation import conv_templates\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "from llava.utils import disable_torch_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_parser(args):\n",
    "    out = args.image_file.split(args.sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "model_path = \"LLaVA-Phi-3-mini-4k-instruct\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt in Phi3 Format\n",
    "qs = \"Describe the image in detail\"\n",
    "conv_mode = \"llava_v0\"\n",
    "\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "if IMAGE_PLACEHOLDER in qs:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "    else:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = image_token_se + \"\\n\" + qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display the image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "! wget http://images.cocodataset.org/val2017/000000281759.jpg\n",
    "\n",
    "img = mpimg.imread('000000281759.jpg')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi3-V 모델에 넣기 위한 전처리\n",
    "\n",
    "image_name = \"000000281759.jpg\"\n",
    "image_files = [image_name]\n",
    "images = load_images(image_files)\n",
    "image_sizes = [x.size for x in images]\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the response\n",
    "\n",
    "temperature = 0.2\n",
    "top_p = 0.7\n",
    "num_beams = 1\n",
    "max_new_tokens = 512\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "outputs = outputs.replace(\"<|end|>\", \"\").strip()\n",
    "print(f\"\\n{outputs}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
