{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0be8c7",
   "metadata": {},
   "source": [
    "## 문장 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8c3a7e-c859-44ab-adff-daebdb86cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The tower is 324 meters (1,063 ft) tall, about the same height as an 81-strorey building, and the tallest structure in Paris. Its base is sqaure, measuring 125 meters (410 ft) on each side. The tower is located on the banks of the River Seine.'}]\n"
     ]
    }
   ],
   "source": [
    "from env_setting import load_dotenv, query\n",
    "\n",
    "load_dotenv()\n",
    "MODEL_URL = \"facebook/bart-large-cnn\"\n",
    "data = query(\n",
    "    MODEL_URL\n",
    "    , \n",
    "    {\n",
    "        \"inputs\": \"The tower is 324 meters (1,063 ft) tall, about the same height as an 81-strorey building, and the tallest structure in Paris. Its base is sqaure, measuring 125 meters (410 ft) on each side.\"\n",
    "        , \"parameters\": {\"do_sample\": False},\n",
    "    }\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c33038",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "답변의 적합성(score)과 answer 답변을 내려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab9baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9326568841934204, 'start': 11, 'end': 16, 'answer': 'Clara'}\n"
     ]
    }
   ],
   "source": [
    "from env_setting import load_dotenv, query\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_URL = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "data = query(\n",
    "    MODEL_URL,\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What's my name?\",\n",
    "            \"context\": \"My name is Clara and I live in Berkeley.\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211305c",
   "metadata": {},
   "source": [
    "## Sentence Similarirty task\n",
    "- 문장의 유사도 검사\n",
    "- 하나의 문장과 여러 문장들의 임베딩 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ffd08c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6945773363113403, 0.9429150819778442, 0.2568760812282562]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from env_setting import load_dotenv, query\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_URL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "data = query(\n",
    "    MODEL_URL,\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": \"That is a happy person\",\n",
    "            \"sentences\": [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9f797",
   "metadata": {},
   "source": [
    "## Text  Classification task    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a398fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'POSITIVE', 'score': 0.9998738765716553},\n",
       "  {'label': 'NEGATIVE', 'score': 0.00012611244164872915}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감성분석 - 긍정인지 부정인지\n",
    "from env_setting import load_dotenv, query\n",
    "\n",
    "load_dotenv()\n",
    "MODEL_URL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "data = query(\n",
    "    MODEL_URL,\n",
    "    {\"inputs\": \"I like you. I love you\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18e1a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The answer to the universe is less straightforward. Pseudobiotic protists get healthier and live longer. (Tweeted by Eric)\\n\\nAs far as I can tell, Ptolemy has not chosen to follow a physician/organist diet or select to take one. Instead, his goal is to learn how to heal. Enjoy the dinner, remember to buy intranasal food (Ditto, etc.) and take your yoga lessons out to the ranch to see what I'm talking about. It's a great way to add up your yoga-related dreams. I am a huge Jay its glad you enjoyed you on a Soylent meal (Besides, I think you have to see Kim least like her on migraines for Dallas, though you are right..). I have always in many ways confirmed I am fine (despite the labels lol) so, I still would consider it an improvement.\\n\\nThat is not to say that my built-in 1-weekly calorie count does anything other than finish most mornings. But, this translation proved to be so unmanageable that during experiments, they lowered my score to -10.\\n\\nWell, there you have it. My 40 man 43 month old Youtube stream has been pretty taxing since maybe that 60-minute video is a communication pill...\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 완성 - 문장이 주어지면 뒤에 문장 생성\n",
    "from env_setting import load_dotenv, query\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_URL = \"openai-community/gpt2\"\n",
    "data = query(\n",
    "    MODEL_URL, \n",
    "    {\n",
    "        \"inputs\": \"The answer to the universe is\"\n",
    "    }\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808ba489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hi, nice to meet you.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 번역기 (러시아어 -> 영어)\n",
    "from env_setting import load_dotenv, query\n",
    "\n",
    "load_dotenv()\n",
    "MODEL_URL = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "data = query(\n",
    "    MODEL_URL,\n",
    "    {\n",
    "        \"inputs\": \"привет, приятно познакомиться\"\n",
    "    }\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1825bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 6\u001b[0m questionAnsweringModel \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-cased-distilled-squad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mAfter 146 days on strike, the guild got most of what it wanted, including increases in compensation for streaming \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mcontent, concessions from studios on minimum staffing for television shows and guarantees.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow long was the strike?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/패캠slm/slm_practice/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Desktop/패캠slm/slm_practice/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# 챗봇 - 질의응답에 맞게 응답\n",
    "# pytorch 설치 필요 -  poetry add torch torchvision torchaudio\n",
    "# tensorflow 설치 필요 - poetry add tensorflow\n",
    "import sentencepiece\n",
    "from transformers import pipeline\n",
    "questionAnsweringModel = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "context = \"\"\"\n",
    "After 146 days on strike, the guild got most of what it wanted, including increases in compensation for streaming \n",
    "content, concessions from studios on minimum staffing for television shows and guarantees.\n",
    "\"\"\"\n",
    "\n",
    "query = \"How long was the strike?\"\n",
    "questionAnsweringModel(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32008d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "Response Content: b'{\"text\":\"THIS IS A SAMPLE ADIOPHILE IT IS GENERATED FOR TESTING PURPOSES\"}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'THIS IS A SAMPLE ADIOPHILE IT IS GENERATED FOR TESTING PURPOSES'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음성인식 - 16Hz .flac 파일\n",
    "# tts_to_flac.py로 음성파일 생성가능\n",
    "import requests\n",
    "import json\n",
    "from env_setting import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGING_FACE_KEY')}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h\"\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "     # 디버깅: 응답 확인\n",
    "    print(\"Response Status Code:\", response.status_code)\n",
    "    print(\"Response Content:\", response.content)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "data = query(\"sample1.flac\")\n",
    "data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
