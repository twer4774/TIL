{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 한국어 데이터 파인튜닝\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_availabe() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 GPU에 올리기\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 상태 확인 - 답변을 못함\n",
    "prompt = \"안국동에 대해 알려줘\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "generate_ids=model.generate(inputs.input_ids, max_length=50, attention_mask=inputs.attenstion_mask)\n",
    "tokenizer.batch_decoe(generate_ids, skip_special_tokens=True, clean_up_tokenization_space=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatsetDic\n",
    "raw_dataset = load_dataset(\"nlpai-lab/kullm-v2\", split=\"train\") # 허깅페이스 한국어 관련 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[\"output\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 dataset 출력 5만줄 가져오기\n",
    "sampled_dataset = raw_dataset.select(range(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 meta는 범용적이므로 한국어 특화 모델을 이용하는 것이 좋음\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('psymon/KoLlama2-7b')\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing 확인\n",
    "sampled_text = \"반갑습니다\"\n",
    "tokenizer.tokenize(sampled_text)\n",
    "\n",
    "# 숫자로 변환, 문장 시작 토큰은 1, 문장 끝 토큰은 2인 경우가 많음\n",
    "tokenizer(sampled_text, return_length=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "def tokenize(batch) :\n",
    "    outputs = tokenizer(\n",
    "        batch['output'],\n",
    "        max_length=context_length,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "\n",
    "    input_batch=[]\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_dataset\n",
    "\n",
    "tokenized_datasets = sampled_dataset.map(tokenize, batched=True, remove_columns=raw_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 상태 확인\n",
    "prompt = \"안국동에 대해 알려줘\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "generate_ids=model.generate(inputs.input_ids, max_length=50, attention_mask=inputs.attenstion_mask)\n",
    "tokenizer.batch_decoe(generate_ids, skip_special_tokens=True, clean_up_tokenization_space=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator 불러오기\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pd_token=tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator에 Data load 확인\n",
    "out = data_collator([tokenized_datasets[i] for i in range(3)])\n",
    "out['input_ids'][0][:20], out['attention_mask'][0][:20], out['labels'][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train arguments 입력\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "#batch_size=32\n",
    "batch_size = 16\n",
    "#logging_steps = 100\n",
    "logging_steps = 10\n",
    "#learing_rate=5e-4\n",
    "learing_rate=3e-3\n",
    "num_epochs=1\n",
    "args = TrainingArguments(\n",
    "    output_idr='/content/drive/myDrive/gdrive/llama_result/testllama', # 학습 결과(모델. 체크포인트 로그 등)가 저장될 경로 지정\n",
    "    per_device_train_batch_size=batch_size,       # 학습 시 사용되는 디바이스(예: GPU) 당 배치 크기 지정\n",
    "    per_device_eval_batch_size=batch_size,        # 평가 시 사용되는 디바이스 당 배치 크기 지정\n",
    "    logging_steps=logging_steps,                  # 몇 스텝마다 로그를 기록할지 지정\n",
    "    save_steps=logging_steps,                     # 몇 스텝마다 모델 체크포인트를 저장할지 지정\n",
    "    gradient_accumulation_steps=8,                # 그라디언트 누적 스텝 수를 지정. 이를 통해 더 큰 가상 배치 크기를 사용\n",
    "    num_train_epochs=num_epochs,                  # 전체 학습 데이터셋을 몇 번 반복할지 지정\n",
    "    weight_decay=0.1,                             # 가중치 감쇠율을 지정. 이는 모델의 과적합을 방지하는데 도움\n",
    "    warmup_steps=logging_steps,                   # 학습 초기의 워밍업 단계에서 사용할 스텝 수 지정\n",
    "    lr_scheduler_type='cosine',                   # 학습률 스케줄러의 타입을 지정. 여기서는 'cosine' 스케줄러 사용\n",
    "    learning_rate=learning_rate,                  # 초기 학습률 지정\n",
    "    fp16=True,                                    # FP16(반 정밀도) 연산을 사용하여 훈련 속도를 높이고 메모리 사용량을 감소\n",
    "    push_to_hub=False                             # 허깅페이스에 푸시할 지 여부\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습기\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrianed('pre_llama')\n",
    "tokenizer.save_pretrained('pre_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('wonik-hi/llama_pre_model')\n",
    "tokenizer.push_to_hub('wonik/hi/llama_pre_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한번 더 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# hugging space에서 dataset 가져오기\n",
    "raw_dataset = load_dataset(\"maywell/ko_wikidata_QA\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 데이터 소스 가져오기\n",
    "raw_dataset = load_dataset(\"maywell/ko_wikidata_QA\", split=\"train\")\n",
    "\n",
    "# 데이터셋 분할 비율 설정\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.1) # 10%를 테스트 세트로 분할\n",
    "\n",
    "# train 데이터셋에서 다시 10%를 검증 세트(validation set)로 분할\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# 최종 데이터셋 구성\n",
    "dataset = DatasetDict({\n",
    "    'train': train_validation_split['train'],\n",
    "    'test': train_test_split['test'],\n",
    "    'validation' : train_validation_split['test'],\n",
    "})\n",
    "\n",
    "# 각 데이터셋의 크기 확인\n",
    "dataset_sizes = {split: len(dataset[split]) for split in dataset.keys()}\n",
    "print(dataset_sizes)\n",
    "\n",
    "sampled_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": dataset['train'].select(range(10000)).shuffle(),\n",
    "        \"valid\": dataset['test'].select(range(1000)).shuffle()\n",
    "    }\n",
    ")\n",
    "\n",
    "sampled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이즈\n",
    "context_length=128\n",
    "\n",
    "def tokenize(batch):\n",
    "    outputs = tokenizer(\n",
    "        batch['output'],\n",
    "        max_length=context_length,\n",
    "        truncation=True,            # true 설정 시, max_lenggth를 초과하는 텍스트 자름\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "\n",
    "    input_batch=[]\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length==context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전체 토크나이징\n",
    "tokenized_datasets = sampled_dataset.map(tokenize, batched=True, remove_columns=raw_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT LoRA 불러오기\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pre_llama\")\n",
    "model = LlamaForCausalLM.from_pretrianed(\"pre_llama\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 라이브러리\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM # 모델의 작업 유형 지정. TaskType.CAUSAL_LM으로 설정하여 인과 언어 모델링 작업 수행\n",
    "    , inference_model=False      # 추론 모드 설정 (False로 설정하여 학습 모드로 설정)\n",
    "    , r=4                        # 로우랭크 크기. 매개변수의 효율성을 높이기 위해 사용되는 저차원 행렬의 랭크를 의미.\n",
    "    , lora_alpha=16              # PEFT의 추론 간섭정도. 로우랭크 행렬의 스케일링 팩터로, 모델의 학습 및 추론 성능에 영향을 미침.\n",
    "    , lora_dropout=0.1           # 드로방웃 비율 설정. 과적합을 방지하기 위해 뉴런의 일부를 무작위로 비활성화하는 기법\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 학습 수 확인\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터에 데이터 올리기\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Argument 입력\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args=TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/gdrive/llama_result/pre_llama\",\n",
    "    per_device_train_batch_size=4, # 데이터 배치 사이즈\n",
    "    logging_steps=500,             # 훈련에서 로깅할 단계\n",
    "    gradient_accumulation_steps=8,  # 8단걔마다 w 조정\n",
    "    num_train_epochs=1,            # 전체 훈련 데이터세트 반복 횟수\n",
    "    weight_decay=0.1,              # w를 10%씩 손실을 고의로 일으키며, overfitting을 방지한다.\n",
    "    lr_scheduler_type='cosine',      # LR 변화를 코사인 함수 형태로 변화\n",
    "    learning_rate=5e-4,             # 학습률\n",
    "    save_steps=1000,                # 기록 저장 스텝\n",
    "    fp16=True,                      # 16비트 부동소수점 연산(True:메모리 사용량 감소, 속도 증가)\n",
    "    push_to_hub=False,              # 허깅페이스 공유 여부\n",
    ")\n",
    "\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습하기\n",
    "trainer.train()\n",
    "\n",
    "# 저장하기\n",
    "model.save_pretrained(\"/content/drive/MyDrive/gdrive/llama_result/peft_llama_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prellama + Peft\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "base_model = LlamaForCausalLM.from_pretrained('pre_llama')\n",
    "model_load=PeftModel.from_pretrained(base_model, 'peft_llama_adapter')\n",
    "model_load.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세팅 - 모델들을 머지 하는 방법 : 두 번의 학습효과\n",
    "model = model_load.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 용량 확인\n",
    "import os\n",
    "os.start('peft_llama_adapter/adapter_model.safetensors').st_size/(1024*1924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델에 쿼리 날려보기\n",
    "question = \"알고리즘 분석\"\n",
    "prompt=f\"\"\"{question}\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs.to(device)\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
