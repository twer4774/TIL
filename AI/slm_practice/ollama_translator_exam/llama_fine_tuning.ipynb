{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파인튜닝을 위한 환경 및 라이브러리 셋업\n",
    "\n",
    "## 필요 라이브러리\n",
    "- datasets : 오디오나 컴퓨터비전, 자연어 처리 작업을 위한 데이터셋을 쉽게 엑세스하고 공유할 수 있는 라이브러리\n",
    "- bitsandbytes : cuda에서 제공하는 함수. 8비트, 4비트 양자화를 위한 라이브러리.\n",
    "    - 양자화(quantization)\n",
    "        - 모델의 성능과 효율성 향상을 위해 Deep Learning의 weight, activation function 출력을 더 작은 비트로 표현하도록 변환하는 기술\n",
    "        - 모델 크기 감소, 계산 속도 향상, 메모리 사용량을 감소하여 효율적인 모델 배포와 실행을 가능하게 해주는 중요한 방법\n",
    "        - 채널 수가 줄어드는 만큼 손실 발생\n",
    "- peft : Parameter-Efficient Fine-Tuning. 적은수의 파라미터만 파인튜닝.\n",
    "- accelerate : GPU 가속기 \n",
    "- Wandb : 훈련을 하면서 발생할 수 있는 로그들 볼 수 있는 툴.\n",
    "- sentense trainsformers : text, embeddings 라이브러리\n",
    "\n",
    "## Trainer, Config, Argument\n",
    "- Trainer : PyTorch에서 제공하는 라이브러리. 모델이 훈련되는 방식을 사용자가 정의해서 사용. Seq2SeqTrainer, Seq2SeqTrainingArguments를 많이 사용함\n",
    "    - TRL : tranformer 언어 모델 관련 풀스택 라이브러리\n",
    "        - Model Classes\n",
    "        - SFTTrainer\n",
    "        - RewardTrainer\n",
    "        - PPOTrainer\n",
    "        - Best-of_N sampling\n",
    "        - DPOTrainer\n",
    "        - TextEnvironment\n",
    "    - STRTrainer : Supervised Fine-tuning Trainer\n",
    "    - DPOTrainer : Data Colloection, Optimization\n",
    "    - ORPO Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\" : \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "terminators = [\n",
    "  pipeline.tokenzier.eos_token,\n",
    "  pipeline.tokenizer.convert_tokens_to_string([\"<|eot_id|>])\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "eos_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "outputs=pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4로도 성능이 안나와서 다른 유형 선택해야함\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning PEFT LoRA 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-recipes ipywidgets # ipywidgets : 노트북과 ipython 커널을 위한 대화형 html 위젯\n",
    "\n",
    "!pip install -q llama-recipies\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from llama_recipes.configs import train_config as TRAIN_CONFIG\n",
    "\n",
    "train_config = TRAIN_CONFIG()\n",
    "train_config.model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "train_config.num_epochs = 1                             # epoch 수\n",
    "train_config.run_validation = False                     # 학습 과정에서 validation 수행 여부\n",
    "train_config.gradirent_accumulation_steps=4             # 그라이언트 누적 스텝 수\n",
    "train_config.batch_size_training=1                      # 학습 시 사용하는 배치 크기\n",
    "train_config.l4=3e-4                                    # 학습률\n",
    "train_config.use_fast_kernels=True                      # Fast Kenrnel 사용 여부\n",
    "train_config.use_fp16=True                              # FP16 연산 사용 여부\n",
    "train_config.context_length=1024 if torch.cuda.get_device_properties(0).total_memory<16e9 else 2024 # T4 16GB or A10 24GB\n",
    "train_config.batching_strategy = \"packing\"              # 배치 전략\n",
    "train_config.output_dir = \"/content/drive/MyDrive/gdrive/llama_result\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig             # 8비트 양자화\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8biyt=True\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    train_config.model_name,\n",
    "    quantization_config=config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 모델 체크\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 요청 프롬프트 \n",
    "# 단점 : 아직 영어만 가능함\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")    # 토크나이저를 사용하여 텍스트 데이터를 토크나이징, pytorch텐서로 변환해서 gpu로 이동\n",
    "\n",
    "model.eval()                                                              # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():                                                     # 그라디언트 계산을 비활성화. 메모리 사용량을 줄이고 연산속도 향상. 평가 및 추론 시 사용\n",
    "  print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.configs.datasets import samsum_dataset\n",
    "from llama_recipes.data.concatenator import ConcatDataset\n",
    "from llama_recipes.utils.config_utils import get_dataloader_kwargs\n",
    "from llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n",
    "\n",
    "samsum_dataset.trust_remote_code=True\n",
    "train_dataset = get_preprocessed_dataset(tokenizer, samsum_dataset, 'train')                  # 전처리된 데이터셋 생성\n",
    "\n",
    "train_dl_kwargs = get_dataloader_kwargs(train_config, train_dataset, tokenizer, \"train\")      # 데이터로더 설정\n",
    "\n",
    "if train_config.batching_strategy == \"packing\":                                               # 패킹 전략 사용\n",
    "        train_dataset = ConcatDataset(train_dataset, chunk_size=train_config.context_length)\n",
    "\n",
    "# Create DataLoaders for the training and validation dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(                                               # 데이터 로더 생성\n",
    "    train_dataset,\n",
    "    num_workers=train_config.num_workers_dataloader,\n",
    "    pin_memory=True,\n",
    "    **train_dl_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PEFT를 위한 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "from llama_recipes.configs import lora_config as LORA_CONFIG\n",
    "\n",
    "lora_config = LORA_CONFIG()\n",
    "lora_config.r = 8 # low rank size\n",
    "lora_config.lora_alpha = 31 # LoRA의 스케일링 벡터\n",
    "lora_dropout: float=0.01    # 드롭아웃 비율 0.01\n",
    "\n",
    "peft_config = LoraConfig(**asdict(lora_config)) # PEFT 설정, asdic: lora_config 객체를 딕셔너리 형태로 변환\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) # model을 k-bit 훈련으로 준비, 모델의 특정 부분을 조정하여 k-bit 정밀도를 사용하도록 설정\n",
    "model = get_peft_model(model, peft_config) # PEFT 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 모델 파인 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(                                    # 옵티마이저 설정\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "        )\n",
    "scheduler = StepLR(optimizer, step_size=-1, gamma=train_config.gamma)\n",
    "\n",
    "\n",
    "results = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    None,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_config.gradient_accumulation_steps,\n",
    "    train_config,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    wandb_run=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 CheckPoint 생성\n",
    "- 트레이닝 중간 체크 포인트 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(train_config.output_dir+\"meta-llama-wonik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 모델 학습 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"wonik-hi/meta_llam_peft\", use_auth_token=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
