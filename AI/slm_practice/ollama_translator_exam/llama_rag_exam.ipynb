{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 방식을 이용한 번역기 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate \n",
    "!pip install einops \n",
    "!pip install langchain \n",
    "!pip install xformers \n",
    "!pip install bitsandbytes \n",
    "!pip install sentence_transformers \n",
    "!pip install chromadb \n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "## transformers 버전은 4.34 이상이 되어야합니다. 안될시 여러가지 오류가 발생 위험이 존재합니다.\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 설정을 위해 라이브러리 변경\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")\n",
    "# print(cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config = model_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "time_start = time()\n",
    "\n",
    "# model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    ")\n",
    "pipeline.model.eval()\n",
    "\n",
    "prompt = \"You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.\"\n",
    "question = \"서울의 유명한 관광 코스를 만들어줄래?\"\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "            {\"role\": \"system\", \"content\": f\"{question}\"}\n",
    "            ]\n",
    "\n",
    "chat_prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    chat_prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset upload\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# df = pd.read_csv(\"/content/drive/MyDrive/part4/history.csv\", encoding='utf-8-sig')\n",
    "df = pd.read_csv(\"/rag_csv_data/history.csv\", encoding='utf-8-sig')\n",
    "\n",
    "df = df[['extracted_text', 'translate_text']]\n",
    "df = df[:100]\n",
    "print(df.head(10))\n",
    "loader = DataFrameLoader(df, page_content_column=\"extracted_text\")\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import llamacpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from textwrap import fill\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/rag_csv_data/history.csv\", encoding='utf-8-sig')\n",
    "\n",
    "df = df[['extracted_text', 'translate_text']]\n",
    "df =  df[:100]\n",
    "print(df.heade(10))\n",
    "loader = DataFrameLoader(df, ipage_content_column=\"extracted_text\")\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(df)\n",
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings\n",
    ")\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "model_kwargs = {'device': 'mps'}\n",
    "encode_kwargs = {'normalize_embeddings' : False}\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectordb = Chroma.from_dcouments(documents=all_splits, embedding=embeddings)\n",
    "vectordb.get(include=[\"metadatas\", \"documents\", \"embeddings\"], limit=10, offset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" Please translate the following sentence. Please construct the translation by referring to metadat. \\n\\\"new\\\" British history and Atlantic history in the early 1970\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    return_source_docuemnts=True\n",
    ")\n",
    "retriever\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please translate the following sentences into Korean. Please construct the translation by referring to metadat. \\n\\\"new\\\" British history and Atlantic history in the early 1970\"\n",
    "\n",
    "response = qa(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"source_documents\"])\n",
    "\n",
    "query = \"\\\"new\\\" British history and Atlantic history in the early 1970\"\n",
    "\n",
    "response = qa(query)\n",
    "result = response['source_documents']\n",
    "print(response['source_documents'])\n",
    "\n",
    "result = response['source_documents'][0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_scv(\"/rag_csv_data/train_0.csv\", encoding='utf-8-sig')\n",
    "df = df[['original', 'modern translation']]\n",
    "df = df[:100]\n",
    "print(df.head(10))\n",
    "loader = DataFrameLoader(df, page_content_column=\"modern translation\")\n",
    "df = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings\n",
    ")\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "vectordb.get(include=[\"metadatas\", \"documents\", \"embeddings\"], limit=10, offset=1)\n",
    "\n",
    "query = \"전하 그 덕망을 승히 여기사 벼슬을 돋우어 이조판서로 좌의정을 하게 하시니, 승상이 국은을 감동하여 갈충보국하니 사방에 일이 업고 도적이 없으매 시화연풍하여 나라가 태평하더라\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"전하 그 덕망을 승히 여기사 벼슬을 돋우어 이조판서로 좌의정을 하게 하시니, 승상이 국은을 감동하여 갈충보국하니 사방에 일이 업고 도적이 없으매 시화연풍하여 나라가 태평하더라\"\n",
    "response = qa(query)\n",
    "print(response)\n",
    "\n",
    "response = qa(query)\n",
    "result = response['source_documents']\n",
    "print(response['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = response['source_documents'][0]\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
