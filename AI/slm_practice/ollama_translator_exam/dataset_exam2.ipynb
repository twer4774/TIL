{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 셋 \n",
    "- AI Hub에서 다운로드한 데이터셋은 json 파일이므로 LLM 학습에 적합하지 않음\n",
    "- dataset_exam에서 수행한 parquet로 저장된 파일로 실습함\n",
    "- colab 환경에서 진행하여 허깅페이스에 업로드 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import dataset_dict, load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "default_path = current_dir + \"/ai_hub/\"\n",
    "data = [d for d in os.listdir(default_path) if d != '.DS_Store' and d != 'Save']\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.read_parquet(default_path + '/Save/일반상식문장생성데이터.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet 파일을 텍스트 형식으로 변환\n",
    "# 전처리 1\n",
    "from typing import Text\n",
    "\n",
    "# 구두점이 있는 문장 합치기\n",
    "for i in range(len(filtered_df)):\n",
    "# for i in range(1000):\n",
    "    # 공백 제거\n",
    "    filtered_df['text'][i] = filtered_df['text'][i].strip()\n",
    "\n",
    "    try:\n",
    "        # 마침표 다음 스페이스 -> 문장 하나\n",
    "        if filtered_df['text'][i].endswith('. '):\n",
    "            filtered_df['text'][i]=filtered_df['Text'][i][:-2]\n",
    "        elif filtered_df['text'][i].endswith('.'):\n",
    "            pass\n",
    "        elif filtered_df['text'][i].endswith(' .'):\n",
    "            print(i, '번째 문장')\n",
    "            print(filtered_df['text'[i]])\n",
    "            filtered_df['text'][i] = filtered_df['text'][i][:-2]+'.'\n",
    "\n",
    "            print(filtered_df['text'][i])\n",
    "        else:\n",
    "            print(i, '번째 문장')\n",
    "            print(filtered_df['text'][i])\n",
    "\n",
    "            # 가장 끝에 공백 -> 문장이 완료된 상태이므로 구두점(.) 추가\n",
    "            if filtered_df['text'][i].endswith(' '):\n",
    "                filtered_df['text'][i] = filtered_df['text'][i][:-1]+'.'\n",
    "            else:\n",
    "                filtered_df['text'][i] = filtered_df['text'][i]+'.'\n",
    "\n",
    "            print(filtered_df['text'][i])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "        print(filtered_df['text'][i])\n",
    "\n",
    "        print('################################################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 2 - 특수문자 제거\n",
    "remove_index = []\n",
    "for i in range(len(filtered_df)):\n",
    "    text = filtered_df['text'][i]\n",
    "    if '�' in text:\n",
    "        remove_index.append(i)\n",
    "    elif '삭제.' in text[-5:]:\n",
    "        remove_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행삭제 및 인덱스 정리\n",
    "filtered_df.drop(remove_index, inplace=True)\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 3 - 결측지 제거 : text 필드에 null인 행 제거\n",
    "filtered_df=filtered_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 전처리에서 행이 사라졌으므로 인덱스 리셋\n",
    "filtered_df=filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_element(text):\n",
    "    return len(text.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_elements 필드와 텍스트 필드를 맞추는 작업. 이 예제에서는 .으로 문장 하나를 만드므로 1로 나옴\n",
    "filtered_df['len_elements'] = filtered_df['text'].apply(lambda x: len(x.split('. ')) if isinstance(x, str) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 초기화\n",
    "filtered_df=filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus 필터링\n",
    "- 구두점이 중간에 찍히는 경우 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1=filtered_df[(filtered_df['len_elements'] >= 2)&(filtered_df['len_elements'] < 15 )] # 2이상 15인 경우 카운팅\n",
    "filtered_df_2=filtered_df[filtered_df['len_elements'] < 2] # 정상 문장 카운팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1=filtered_df_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['len_elements'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못된 문장들을 올바르게 고치는 작업\n",
    "filtered_df_1['text'] = ['하나에 열중하니 아무래도, 다른 것에는 소원하게 되었다.', '검은색, 붉은색, 녹색의 다채로운 쌀과, 찹쌀 등을 생산하고 있다.', '이 화장품 회사는 예전부터 꾸준히 1위 자리를 지키고 있다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1['len_elements'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2=filtered_df_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 filtered_df_1을 filtered_df_2와 합침\n",
    "filtered_df_2 = pd.concat([filtered_df_2, filtered_df_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 초기화\n",
    "filtered_df_2=filtered_df_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 추론 테스트\n",
    "- 데이터가 잘 나오는지 확인하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order inference를 진행하는 함수\n",
    "def make_text_allign_data(text):\n",
    "    if not text.endswith('.') or text.endswit('. '):\n",
    "        text += '.'\n",
    "\n",
    "    if text.endswith('.'):\n",
    "        text+=' '\n",
    "    \n",
    "    my_list = text.split('. ')\n",
    "    my_list_with_dots = [element + '.' for element in my_list]\n",
    "\n",
    "    if my_list_with_dots[-1] == '.':\n",
    "        my_list_with_dots=my_list_with_dots[:-1]\n",
    "\n",
    "    random.shuffle(my_list_with_dots)\n",
    "\n",
    "    if len(my_list_with_dots) == 1:\n",
    "        raise Exception(\"allign 리스트 기이가 1입니다.\", text)\n",
    "    \n",
    "\n",
    "    tmp_input='당신은 인공지능 비서입니다. 주어진 문장 리스트를 모두 활용하여 가장 정확한 단락을 생성하세요.'\n",
    "    tmp_inst=f'이 문장들은 임의의 순서로 섞여 있습니다. 모든 문장을 활용하여 원본 단락의 순서와 내용을 올바른 순서로 재구성하세요.\\n#문장 리스트: {my_list_with_dots}'\n",
    "\n",
    "    if text.endswith(' '):\n",
    "        tmp_out=text[:-1]\n",
    "    else:\n",
    "        tmp_out=text\n",
    "\n",
    "    return tmp_input, tmp_inst, tmp_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장이 주어졌을때 마지막 문장 예측하는 함수\n",
    "def make_completion_data(text):\n",
    "  if not (text.endswith('.') or text.endswith('. ')):\n",
    "    text += '.'\n",
    "\n",
    "  if text.endswith('. '):\n",
    "    text=text\n",
    "  else:\n",
    "    text = text +' '\n",
    "\n",
    "  my_list_with_dots = text.split('. ')\n",
    "\n",
    "  if my_list_with_dots[-1] == '':\n",
    "    my_list_with_dots=my_list_with_dots[:-1]\n",
    "\n",
    "    if len(my_list_with_dots) ==1:\n",
    "      raise Exception(\"Completions 텍스트를 나눈 리스트의 길이가 1입니다.\", text)\n",
    "\n",
    "    last_sentence = my_list_with_dots.pop()\n",
    "    last_sentence += '.'\n",
    "\n",
    "  remaining_paragraph=''\n",
    "\n",
    "  for i in range(len(my_list_with_dots)):\n",
    "    if i!=len(my_list_with_dots)-1:\n",
    "                 remaining_paragraph += my_list_with_dots[i]+'. '\n",
    "    else:\n",
    "      remaining_paragraph += my_list_with_dots[i]+'.' # 문장을 기억하고 있도록 설정\n",
    "\n",
    "  tmp_input='당신은 인공지능 비서입니다. 주어진 원문을 바탕으로 주어진 질문에 가장 적절한 답변을 생성하세요.'\n",
    "  tmp_instruct=f'다음 텍스트에서 제공된 문맥을 정확히 이해하고, 마지막 문장을 자연스럽고 문맥에 맞게 완성하세요. 문장은 이전 내용과 논리적으로 연결되어야 합니다.\\n#텍스트: {remaining_paragraph}'\n",
    "  tmp_output=last_sentence\n",
    "\n",
    "  return tmp_input, tmp_instruct, tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 괄호 안에 문장이 들어도록\n",
    "def make_text_mask_data(text):\n",
    "  if not (text.endswith('.') or text.endswith('. ')):\n",
    "    text += '.'\n",
    "\n",
    "  if text.endswith('. '):\n",
    "    text=text[:-1]\n",
    "  else:\n",
    "    text = text\n",
    "\n",
    "  words = re.findall(r'[가-힣]{2,}', text)\n",
    "  random_word = random.choice(words)\n",
    "\n",
    "  masked_text = text.replace(random_word, '<MASK>')\n",
    "\n",
    "  tmp_input='당신은 인공지능 비서입니다. 주어진 질문에 가장 적절한 답변을 제공하세요.'\n",
    "  tmp_instruct=f'이 문제에서는 주어진 텍스트 내의 <MASK>로 표시된 부분에 들어갈 적절한 단어를 예측해야 합니다. <MASK>가 위치한 문장의 전체 문맥을 분석하여, 문장의 나머지 내용과 일관되게 <MASK>에 들어갈 가장 적합한 단어를 답하세요.\\n#텍스트: {masked_text}'\n",
    "  tmp_output=random_word\n",
    "\n",
    "  return tmp_input, tmp_instruct, tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_align(text):\n",
    "  word_lst=[]\n",
    "  for word in text.split(' '):\n",
    "    out = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "    word_lst.append(out)\n",
    "\n",
    "  #중복 제거하기 위해 set으로 만듬\n",
    "  word_lst = set(word_lst)\n",
    "\n",
    "  #다시 리스트 형식으로 돌림\n",
    "  word_lst=list(word_lst)\n",
    "\n",
    "  #랜덤하게 재배열\n",
    "  random.shuffle(word_lst)\n",
    "\n",
    "  tmp_input='당신은 인공지능 비서입니다. 주어진 지시사항에 따라 가장 적절한 문장을 생성하세요.'\n",
    "  tmp_instruct=f'이 문제에는 문장에서 공백을 기준으로 나누고, 구두점을 제거한 무작위로 섞인 단어들이 담긴 리스트가 제공됩니다. 이 리스트의 단어를 모두 활용하여 가장 문맥상 적절한 문장을 생성하세요.\\n#단어리스트: {word_lst}'\n",
    "  tmp_output=text\n",
    "\n",
    "  return tmp_input, tmp_instruct, tmp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성 파이프라인\n",
    "from os import TMP_MAX\n",
    "input_lst=[]\n",
    "output_lst=[]\n",
    "inst_lst=[]\n",
    "id_lst=[]\n",
    "\n",
    "for i in tqdm(range(len(filtered_df_2))):\n",
    "  try:\n",
    "    text=filtered_df_2['text'][i]\n",
    "\n",
    "    thred=random.random()\n",
    "\n",
    "    if thred < 0.56:\n",
    "      TMP_MAXtmp_id='word_align_aihub'\n",
    "    else:\n",
    "      # tmp_input, tmp_instruct, tmp_output = make_text_mask_data(text)\n",
    "      tmp_input, tmp_instruct, tmp_output = make_completion_data(text)\n",
    "      tmp_id='pre_mask_aihub'\n",
    "\n",
    "    input_lst.append(tmp_input)\n",
    "    inst_lst.append(tmp_instruct)\n",
    "    output_lst.append(tmp_output)\n",
    "    id_lst.append(tmp_id)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"{i}번째 행\")\n",
    "    print(e)\n",
    "    print(filtered_df_2['text'][i])\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df_2=pd.DataFrame({'input':input_lst,'instruction':inst_lst,'output':output_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df = hub_df_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df=hub_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_df.to_parquet(default_path+'Save/일반상식문장생성데이터2.parquet', engine = 'pyarrow', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets\n",
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import Dataset\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스에 데이터 셋을 올리려면 판다스 형태로 변환해야함.\n",
    "dataset = Dataset.from_pandas(hub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub('wonik-hi/korea_common_sence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
