{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 어플리케이션을 위한 PDF 파싱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "- pdfminer로 텍스트 추출\n",
    "- GPT-4V를 이용해 PDF를 이미지로 변환 후 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pdf2image\n",
    "%pip install pdfminer\n",
    "%pip install openai\n",
    "%pip install scikit-learn\n",
    "%pip install rich\n",
    "%pip install tqdm\n",
    "%pip install concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "from pdfminer.high_level import extract_text\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import os\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 프로세싱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_images(path):\n",
    "    images = convert_from_path(path)\n",
    "    return images\n",
    "\n",
    "def extract_text_from_doc(path):\n",
    "    text = extract_text(path)\n",
    "    page_text = []\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pdf를 이미지로 변환하는 라이브러리\n",
    "!apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/content/drive/MyDrive/gdrive/data/example_pdfs/fine-tuning-deck.pdf\"\n",
    "file_path = \"/content/drive/MyDrive/example_pdfs/fine-tuning-deck.pdf\"\n",
    "\n",
    "images = convert_doc_to_images(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_doc(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4V를 이용한 이미지 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_uri(img):\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format=\"jpeg\")\n",
    "    base64_image = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    data_uri = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "    return data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You will be provided with an image of a pdf page or a slide. Your goal is to talk about the content that you see, in technical terms, as if you were delivering a presentation.\n",
    "\n",
    "If there are diagrams, describe the diagrams and explain their meaning.\n",
    "For example: if there is a diagram describing a process flow, say something like \"the process flow starts with X then we have Y and Z...\"\n",
    "\n",
    "If there are tables, describe logically the content in the tables\n",
    "For example: if there is a table listing items and prices, say something like \"the prices are the following: A for X, B for Y...\"\n",
    "\n",
    "DO NOT include terms referring to the content format\n",
    "DO NOT mention the content type - DO focus on the content itself\n",
    "For example: if there is a diagram/chart and text on the image, talk about both without mentioning that one is a chart and the other is text.\n",
    "Simply describe what you see in the diagram and what you understand from the text.\n",
    "\n",
    "You should keep it concise, but keep in mind your audience cannot see the image so be exhaustive in describing the content.\n",
    "\n",
    "Exclude elements that are not relevant to the content:\n",
    "DO NOT mention page numbers or the position of the elements on the image.\n",
    "\n",
    "------\n",
    "\n",
    "If there is an identifiable title, identify the title to give the output in the following format:\n",
    "\n",
    "{TITLE}\n",
    "\n",
    "{Content description}\n",
    "\n",
    "If there is no clear title, simply return the content description.\n",
    "\n",
    "'''\n",
    "\n",
    "def analyze_image(img_url):\n",
    "  response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "              {\"type\": \"image_url\",\n",
    "               \"image_url\": {\n",
    "                   \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                   },\n",
    "               },\n",
    "              ],\n",
    "          }\n",
    "      ],\n",
    "  max_tokens=300,\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "    #model=\"gpt-4-vision-preview\",\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": img_url,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "        max_tokens=300,\n",
    "        top_p=0.1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[2]\n",
    "data_uri = get_img_uri(img)\n",
    "data_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_image(data_uri)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 폴더 내 모든 문서 프로세싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = \"/content/drive/MyDrive/example_pdfs\"\n",
    "\n",
    "all_items = os.listdir(files_path)\n",
    "files = [item for item in all_items if os.path.isfile(os.path.join(files_path, item))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_doc_image(img):\n",
    "    img_uri = get_img_uri(img)\n",
    "    data = analyze_image(img_uri)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    path = f\"{files_path}/{f}\"\n",
    "    doc = {\n",
    "        \"filename\": f\n",
    "    }\n",
    "    text = extract_text_from_doc(path)\n",
    "    doc['text'] = text\n",
    "    imgs = convert_doc_to_images(path)\n",
    "    pages_description = []\n",
    "\n",
    "    print(f\"Analyzing pages for doc {f}\")\n",
    "\n",
    "    # Concurrent execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "\n",
    "        # Removing 1st slide as it's usually just an intro\n",
    "        futures = [\n",
    "            executor.submit(analyze_doc_image, img)\n",
    "            for img in imgs[1:]\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(imgs)-1) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "\n",
    "        for f in futures:\n",
    "            res = f.result()\n",
    "            pages_description.append(res)\n",
    "\n",
    "    doc['pages_description'] = pages_description\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "json_path = \"/content/drive/MyDrive/parsed_pdf_docs.json\"\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'r') as f:\n",
    "  docs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Content\n",
    "- 청크 단위로 페이지 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking content by page and merging together slides text & description if applicable\n",
    "content = []\n",
    "for doc in docs:\n",
    "    # Removing first slide as well\n",
    "    text = doc['text'].split('\\f')[1:]\n",
    "    description = doc['pages_description']\n",
    "    description_indexes = []\n",
    "    for i in range(len(text)):\n",
    "        slide_content = text[i] + '\\n'\n",
    "        # Trying to find matching slide description\n",
    "        slide_title = text[i].split('\\n')[0]\n",
    "        for j in range(len(description)):\n",
    "            description_title = description[j].split('\\n')[0]\n",
    "            if slide_title.lower() == description_title.lower():\n",
    "                slide_content += description[j].replace(description_title, '')\n",
    "                # Keeping track of the descriptions added\n",
    "                description_indexes.append(j)\n",
    "        # Adding the slide content + matching slide description to the content pieces\n",
    "        content.append(slide_content)\n",
    "    # Adding the slides descriptions that weren't used\n",
    "    for j in range(len(description)):\n",
    "        if j not in description_indexes:\n",
    "            content.append(description[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in content:\n",
    "    print(c)\n",
    "    print(\"\\n\\n-------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨텐츠가 잘 안나오는 경우가 있으므로 보간 방법으로 해당 코드 실행\n",
    "\n",
    "# Cleaning up content\n",
    "# Removing trailing spaces, additional line breaks, page numbers and references to the content being a slide\n",
    "clean_content = []\n",
    "for c in content:\n",
    "    text = c.replace(' \\n', '').replace('\\n\\n', '\\n').replace('\\n\\n\\n', '\\n').strip()\n",
    "    text = re.sub(r\"(?<=\\n)\\d{1,2}\", \"\", text)\n",
    "    text = re.sub(r\"\\b(?:the|this)\\s*slide\\s*\\w+\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    clean_content.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clean_content:\n",
    "    print(c)\n",
    "    print(\"\\n\\n-------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 형식으로 변경\n",
    "# Creating the embeddings\n",
    "# We'll save to a csv file here for testing purposes but this is where you should load content in your vectorDB.\n",
    "df = pd.DataFrame(clean_content, columns=['content'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"text-embedding-3-large\"\n",
    "\n",
    "def get_embeddings(text):\n",
    "    embeddings = client.embeddings.create(\n",
    "      model=\"text-embedding-3-small\",\n",
    "      input=text,\n",
    "      encoding_format=\"float\"\n",
    "    )\n",
    "    return embeddings.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['content'].apply(lambda x: get_embeddings(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving locally for later\n",
    "data_path = \"/content/drive/MyDrive/parsed_pdf_docs_with_embeddings.csv\"\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load data from saved file\n",
    "df = pd.read_csv(data_path)\n",
    "df[\"embeddings\"] = df.embeddings.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-augmented generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "    You will be provided with an input prompt and content as context that can be used to reply to the prompt.\n",
    "\n",
    "    You will do 2 things:\n",
    "\n",
    "    1. First, you will internally assess whether the content provided is relevant to reply to the input prompt.\n",
    "\n",
    "    2a. If that is the case, answer directly using this content. If the content is relevant, use elements found in the content to craft a reply to the input prompt.\n",
    "\n",
    "    2b. If the content is not relevant, use your own knowledge to reply or say that you don't know how to respond if your knowledge is not sufficient to answer.\n",
    "\n",
    "    Stay concise with your answer, replying specifically to the input prompt without mentioning additional information provided in the context content.\n",
    "'''\n",
    "\n",
    "#model=\"gpt-4-turbo-preview\"\n",
    "model=\"gpt-4o\"\n",
    "\n",
    "\n",
    "# 모델에 검색 요청\n",
    "def search_content(df, input_text, top_k):\n",
    "    embedded_value = get_embeddings(input_text)\n",
    "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(np.array(x).reshape(1,-1), np.array(embedded_value).reshape(1, -1)))\n",
    "    res = df.sort_values('similarity', ascending=False).head(top_k)\n",
    "    return res\n",
    "\n",
    "# 유사도 측정\n",
    "def get_similarity(row):\n",
    "    similarity_score = row['similarity']\n",
    "    if isinstance(similarity_score, np.ndarray):\n",
    "        similarity_score = similarity_score[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "# 응답 생성\n",
    "def generate_output(input_prompt, similar_content, threshold = 0.5):\n",
    "\n",
    "    content = similar_content.iloc[0]['content']\n",
    "\n",
    "    # Adding more matching content if the similarity is above threshold\n",
    "    if len(similar_content) > 1:\n",
    "        for i, row in similar_content.iterrows():\n",
    "            similarity_score = get_similarity(row)\n",
    "            if similarity_score > threshold:\n",
    "                content += f\"\\n\\n{row['content']}\"\n",
    "\n",
    "    prompt = f\"INPUT PROMPT:\\n{input_prompt}\\n-------\\nCONTENT:\\n{content}\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.5,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user queries related to the content\n",
    "example_inputs = [\n",
    "    'What are the main models you offer?',\n",
    "    'Do you have a speech recognition model?',\n",
    "    'Which embedding model should I use for non-English use cases?',\n",
    "    'Can I introduce new knowledge in my LLM app using RAG?',\n",
    "    'How many examples do I need to fine-tune a model?',\n",
    "    'Which metric can I use to evaluate a summarization task?',\n",
    "    'Give me a detailed example for an evaluation process where we are looking for a clear answer to compare to a ground truth.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Running the RAG pipeline on each example\n",
    "for ex in example_inputs:\n",
    "    # 응답 포맷에 스타일 지정\n",
    "    print(f\"[deep_pink4][bold]QUERY:[/bold] {ex}[/deep_pink4]\\n\\n\")\n",
    "    matching_content = search_content(df, ex, 3) # 유사한것 3가지 조회\n",
    "    print(f\"[grey37][b]Matching content:[/b][/grey37]\\n\")\n",
    "    for i, match in matching_content.iterrows():\n",
    "        print(f\"[grey37][i]Similarity: {get_similarity(match):.2f}[/i][/grey37]\")\n",
    "        print(f\"[grey37]{match['content'][:100]}{'...' if len(match['content']) > 100 else ''}[/[grey37]]\\n\\n\")\n",
    "    reply = generate_output(ex, matching_content)\n",
    "    print(f\"[turquoise4][b]REPLY:[/b][/turquoise4]\\n\\n[spring_green4]{reply}[/spring_green4]\\n\\n--------------\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
