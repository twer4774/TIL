{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergekit\n",
    "- LLM을 병합하는 툴킷\n",
    "\n",
    "## Merge Algorithm\n",
    "1. SLERP (Spherical Linear Interpolation)\n",
    "    - 두 벡터를 보간하기 위한 방법\n",
    "    - 두개의 모델로 이용\n",
    "\n",
    "2. TIES\n",
    "    - 멀티 태스킹 가능\n",
    "\n",
    "3. DARE\n",
    "    - Pruning, Rescaling 방식이 존재함\n",
    "\n",
    "4. Passthrough\n",
    "    - 덩치가 큰 모델을 만드는 방법 (레이어를 합쳐서 수행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 Merge 방법의 yaml 파일\n",
    "\n",
    "### TIES-Merging\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  - model: mistralai/Mistral-7B-v0.1\n",
    "    # no parameters necessary for base model\n",
    "  - model: OpenPipe/mistral-ft-optimized-1218\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.5\n",
    "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.3\n",
    "merge_method: ties\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "parameters:\n",
    "  normalize: true\n",
    "dtype: float16\n",
    "```\n",
    "\n",
    "### SLERP\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: OpenPipe/mistral-ft-optimized-1218\n",
    "        layer_range: [0, 32]\n",
    "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "        layer_range: [0, 32]\n",
    "merge_method: slerp\n",
    "base_model: OpenPipe/mistral-ft-optimized-1218\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "```\n",
    "\n",
    "### Passthrough\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "    - model: OpenPipe/mistral-ft-optimized-1218\n",
    "      layer_range: [0, 32]\n",
    "  - sources:\n",
    "    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "      layer_range: [24, 32]\n",
    "merge_method: passthrough\n",
    "dtype: bfloat16\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
