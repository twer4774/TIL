{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergekit\n",
    "- LLMÏùÑ Î≥ëÌï©ÌïòÎäî Ìà¥ÌÇ∑\n",
    "\n",
    "## Merge Algorithm\n",
    "1. SLERP (Spherical Linear Interpolation)\n",
    "    - Îëê Î≤°ÌÑ∞Î•º Î≥¥Í∞ÑÌïòÍ∏∞ ÏúÑÌïú Î∞©Î≤ï\n",
    "    - ÎëêÍ∞úÏùò Î™®Îç∏Î°ú Ïù¥Ïö©\n",
    "\n",
    "2. TIES\n",
    "    - Î©ÄÌã∞ ÌÉúÏä§ÌÇπ Í∞ÄÎä•\n",
    "\n",
    "3. DARE\n",
    "    - Pruning, Rescaling Î∞©ÏãùÏù¥ Ï°¥Ïû¨Ìï®\n",
    "\n",
    "4. Passthrough\n",
    "    - Îç©ÏπòÍ∞Ä ÌÅ∞ Î™®Îç∏ÏùÑ ÎßåÎìúÎäî Î∞©Î≤ï (Î†àÏù¥Ïñ¥Î•º Ìï©Ï≥êÏÑú ÏàòÌñâ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Í∞Å Merge Î∞©Î≤ïÏùò yaml ÌååÏùº\n",
    "\n",
    "### TIES-Merging\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  - model: mistralai/Mistral-7B-v0.1\n",
    "    # no parameters necessary for base model\n",
    "  - model: OpenPipe/mistral-ft-optimized-1218\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.5\n",
    "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.3\n",
    "merge_method: ties\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "parameters:\n",
    "  normalize: true\n",
    "dtype: float16\n",
    "```\n",
    "\n",
    "### SLERP\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: OpenPipe/mistral-ft-optimized-1218\n",
    "        layer_range: [0, 32]\n",
    "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "        layer_range: [0, 32]\n",
    "merge_method: slerp\n",
    "base_model: OpenPipe/mistral-ft-optimized-1218\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "```\n",
    "\n",
    "### Passthrough\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "    - model: OpenPipe/mistral-ft-optimized-1218\n",
    "      layer_range: [0, 32]\n",
    "  - sources:\n",
    "    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "      layer_range: [24, 32]\n",
    "merge_method: passthrough\n",
    "dtype: bfloat16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cg123/mergekit.git\n",
    "!cd mergekit && pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "MODEL_NAME = \"Marcoro14-7B-slerp_v2\"\n",
    "yaml_config = \"\"\"\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: OpenPipe/mistral-ft-optimized-1218\n",
    "        layer_range: [0, 32]\n",
    "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "        layer_range: [0, 32]\n",
    "merge_method: slerp\n",
    "base_model: OpenPipe/mistral-ft-optimized-1218\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "\"\"\"\n",
    "\n",
    "with open('config.yaml', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Models\n",
    "!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU huggingface_hub\n",
    "\n",
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "from jinja2 import Template\n",
    "\n",
    "usernmae = \"wonik-hi\"\n",
    "\n",
    "template_text = \"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- merge\n",
    "- mergekit\n",
    "- lazymergekit\n",
    "{%- for model in models %}\n",
    "- {{ model }}\n",
    "{%- endfor %}\n",
    "---\n",
    "\n",
    "# {{ model_name }}\n",
    "\n",
    "{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
    "\n",
    "{%- for model in models %}\n",
    "* [{{ model }}](https://huggingface.co/{{ model }})\n",
    "{%- endfor %}\n",
    "\n",
    "## üß© Configuration\n",
    "\n",
    "```yaml\n",
    "{{- yaml_config -}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())\n",
    "\n",
    "# Get list of models from config\n",
    "data = yaml.safe_load(yaml_config)\n",
    "if \"models\" in data:\n",
    "    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
    "elif \"parameters\" in data:\n",
    "    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
    "elif \"slices\" in data:\n",
    "    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
    "else:\n",
    "    raise Exception(\"No models or slices found in yaml config\")\n",
    "\n",
    "\n",
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "    model_name=MODEL_NAME,\n",
    "    models=models,\n",
    "    yaml_config=yaml_config,\n",
    "    username=username,\n",
    ")\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save('merge/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{username}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.coalb import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"wonik-hi\"\n",
    "\n",
    "api = HfAPi(token=userdata.get(\"HF_API_KEY2\"))\n",
    "\n",
    "\"\"\"\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username}/MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\"\"\"\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIES Merge Î∞©Î≤ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "MODEL_NAME = \"wonik-hi_merge_model\"\n",
    "yaml_config = \"\"\"\n",
    "models:\n",
    "  - model: wonik-hi/Marcoro14-7B-slerp\n",
    "    parameters:\n",
    "      density: [1, 0.7, 0.1] # density gradient\n",
    "      weight: 1.0\n",
    "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "    parameters:\n",
    "      density: 0.33\n",
    "      weight:\n",
    "        - filter: mlp\n",
    "          value: 0.5\n",
    "        - value: 0\n",
    "merge_method: ties\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "parameters:\n",
    "  normalize: true\n",
    "  int8_mask: true\n",
    "dtype: float16\n",
    "name: gradient-slerp-ties\n",
    "\"\"\"\n",
    "\n",
    "# Save config as yaml file\n",
    "with open('/content/conf.yml', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î≥ÄÏàò ÏÖãÌåÖ\n",
    "OUTPUT_PATH = \"./merged_v2\" # Ï†ÄÏû•Îê† Í≤ΩÎ°ú\n",
    "LORA_MERGE_CACHE = \"/tmp\" # Î°úÎùº Î≥ëÌï© Ï∫êÏãú Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "CONFIG_YML = \"/content/conf.yml\" # config yml ÌååÏùº Í≤ΩÎ°ú\n",
    "COPY_TOKENIZER = True\n",
    "LAZY_UNPICKLE = True # Ïã§ÌóòÏ†ÅÏù∏ Ï†ÄÏö©Îüâ Î™®Îç∏ Î°úÎçî Í∏∞Îä• ÌôúÏÑ±Ìôî\n",
    "LOW_CPU_MEMORY = True # ÏÑúÎ≤Ñ ÏÑ±Îä• Ï¢ãÏúºÎ©¥ True ÏÑ§Ï†ï\n",
    "ALLOW_CRIMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cg123/mergekit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd mergekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÍπÉÏóê ÏÖãÌåÖÎêú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ìå®ÌÇ§ÏßÄ ÏûÑÌè¨Ìä∏\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "# yml ÌååÏùº ÏÑ§Ï†ïÍ∞í Î∂àÎü¨Ïò§Í∏∞\n",
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(), # GPU ÏÑ∏ÌåÖ\n",
    "        copy_tokenizer=COPY_TOKENIZER, # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "        allow_crimes=ALLOW_CRIMES,\n",
    "    ),\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"wonik-hi\"\n",
    "\n",
    "api = HfApi(token=userdata.get(\"huggingface\"))\n",
    "\n",
    "\"\"\"\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"merged_v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- Ïã§ÌñâÌôòÍ≤Ω : T4 Ï†ïÎèÑÏùò GPU ÏÑ±Îä• ÌïÑÏöî\n",
    "- KoBEST : Korean Balanced Evaluation of Singificant Tasks\n",
    "Ïûò Íµ¨ÏÑ±Îêú Î≤§ÏπòÎßàÌÅ¨Îäî Îã§ÏñëÌïú Î™®Îç∏ÏùÑ Í∞ùÍ¥ÄÏ†ÅÏù¥Í≥† Ï†ïÎ∞ÄÌïòÍ≤å ÌèâÍ∞Ä -> ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ Î∂ÑÏïºÏùò Î∞úÏ†ÑÏùÑ Ï¥âÏßÑ\n",
    "\n",
    "- Ko LM Eval Harness : ÌïúÍµ≠Ïñ¥ Í≥µÍ∞ú Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú ÌèâÍ∞ÄÌïòÎäî Ïñ∏Ïñ¥ Î™®Îç∏ Ï†êÏàò\n",
    "    - ÌÅ¨Í∏∞Í∞Ä ÌÅ¨Í∏∞ ÎïåÎ¨∏Ïóê colabÏóêÏÑú ÏßÑÌñâÌïòÎ©¥ ÎÅäÍ∏∏ Í∞ÄÎä•ÏÑ±Ïù¥ Îß§Ïö∞ ÎÜíÏùå.\n",
    "    - Ïã§Ìñâ Ïãú Î°úÏª¨Î°ú ÎÇ¥Î¶¨Í≥† Ïã§ÌñâÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï®.\n",
    "    - Î™®Îì† ÌèâÍ∞ÄÎ•º ÌïúÎ≤àÏóê ÏßÑÌñâÌïòÎäî Í≤ÉÏùÄ ÎÑàÎ¨¥ Ïò§Îûò Í±∞Î¶¥ÎØÄÎ°ú, ÌÉúÏä§ÌÅ¨Î•º ÎÇòÎà†ÏÑú ÏßÑÌñâÌïòÎèÑÎ°ù ÌïòÎäîÍ≤ÉÏùÑ Í∂åÏû•.\n",
    "``` shell\n",
    "git clone https://github.com/Beomi/ko-lm-evalutation-harness\n",
    "cd ko-lm-evaluation-harness\n",
    "pip isntall -r requirements.txt\n",
    "./run_all.sh Î™®Îç∏Ïù¥Î¶Ñ 'GPUÎ≤àÌò∏Îì§'\n",
    "\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # Ïù¥Î†áÍ≤å ÌïòÎ©¥ GPU 0,1Î≤àÏóê 'eomi/llama-2-ko-7b' Î™®Îç∏ÏùÑ Ï™ºÍ∞úÏÑú Ïò¨Î¶¨Í≥† ÌèâÍ∞ÄÌïúÎã§.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Ko BEST Dataset\n",
    "huggingface.co/datasets/skt/kobest_v1/viewer/boolq/train\n",
    "\n",
    "\n",
    "### Ko BEST Eval Code\n",
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/new_mistral7B,use_accelerate=true, trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/new_mistral7B/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Beomi/ko-lm-evaluation-harness\n",
    "%cd ko-lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "#./run_all.sh Î™®Îç∏Ïù¥Î¶Ñ 'GPUÎ≤àÌò∏Îì§'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏàòÌñâ ÏãúÍ∞Ñ Ïò§Îûò Í±∏Î¶º Ï£ºÏùò!!!\n",
    "! ./run_all.sh 'wonik-hi/phi-3-mini-QLoRA' '0'\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # Ïù¥Î†áÍ≤å ÌïòÎ©¥ GPU0,1Î≤àÏóê 'beomi/llama-2-ko-7b' Î™®Îç∏ÏùÑ Ï™ºÍ∞úÏÑú Ïò¨Î¶¨Í≥† ÌèâÍ∞ÄÌïúÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏàòÌñâ ÏãúÍ∞Ñ Ïò§Îûò Í±∏Î¶º Ï£ºÏùò!!!\n",
    "# ./run_all.sh model GPU_NO few-shot test\n",
    "! ./run_all.sh 'oz1115/phi-3-mini-QLoRA' '0' \"0\" 0.2\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # Ïù¥Î†áÍ≤å ÌïòÎ©¥ GPU0,1Î≤àÏóê 'beomi/llama-2-ko-7b' Î™®Îç∏ÏùÑ Ï™ºÍ∞úÏÑú Ïò¨Î¶¨Í≥† ÌèâÍ∞ÄÌïúÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixtral eval\n",
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/new_mistral7B,use_accelerate=true,trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/new_mistral7B/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/phi-3-mini-QLoRA,use_accelerate=true,trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/phi-3-mini-QLoRA2/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# results/korean_origin_bench/20b/00_shot.json\n",
    "def _get_metric_name(v):\n",
    "    metrics = ['f1', 'macro_f1', 'acc_norm', 'acc']\n",
    "    for m in metrics:\n",
    "        if v.get(m):\n",
    "            return {\n",
    "                'metric': m,\n",
    "                'value': v[m],\n",
    "            }\n",
    "\n",
    "def get_df_klue(path, model_name=''):\n",
    "    data = []\n",
    "    for i in ['0', '5', '10', '50']:\n",
    "        shot = f'{path}/{i}_shot.json'\n",
    "        try:\n",
    "            data.append(\n",
    "                {\n",
    "                    f\"{k} ({_get_metric_name(v)['metric']})\": _get_metric_name(v)['value']\n",
    "                    for k, v in json.load(open(shot))['results'].items()\n",
    "                }\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    df = pd.DataFrame(data, index=[0, 5, 10, 50][:len(data)]).T\n",
    "    print(df.to_markdown())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_klue('results/klue_etc_bench/home/jovyan/beomi/llama2-koen-13b/60b', 'llama2-koen-13b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_models = sorted(glob('results/all/*/*'))\n",
    "various_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in various_models:\n",
    "    print(model)\n",
    "    get_df_klue(model)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
