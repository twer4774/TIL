{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergekit\n",
    "- LLMì„ ë³‘í•©í•˜ëŠ” íˆ´í‚·\n",
    "\n",
    "## Merge Algorithm\n",
    "1. SLERP (Spherical Linear Interpolation)\n",
    "    - ë‘ ë²¡í„°ë¥¼ ë³´ê°„í•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "    - ë‘ê°œì˜ ëª¨ë¸ë¡œ ì´ìš©\n",
    "\n",
    "2. TIES\n",
    "    - ë©€í‹° íƒœìŠ¤í‚¹ ê°€ëŠ¥\n",
    "\n",
    "3. DARE\n",
    "    - Pruning, Rescaling ë°©ì‹ì´ ì¡´ì¬í•¨\n",
    "\n",
    "4. Passthrough\n",
    "    - ë©ì¹˜ê°€ í° ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²• (ë ˆì´ì–´ë¥¼ í•©ì³ì„œ ìˆ˜í–‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê° Merge ë°©ë²•ì˜ yaml íŒŒì¼\n",
    "\n",
    "### TIES-Merging\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  - model: mistralai/Mistral-7B-v0.1\n",
    "    # no parameters necessary for base model\n",
    "  - model: OpenPipe/mistral-ft-optimized-1218\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.5\n",
    "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "    parameters:\n",
    "      density: 0.5\n",
    "      weight: 0.3\n",
    "merge_method: ties\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "parameters:\n",
    "  normalize: true\n",
    "dtype: float16\n",
    "```\n",
    "\n",
    "### SLERP\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: OpenPipe/mistral-ft-optimized-1218\n",
    "        layer_range: [0, 32]\n",
    "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "        layer_range: [0, 32]\n",
    "merge_method: slerp\n",
    "base_model: OpenPipe/mistral-ft-optimized-1218\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "```\n",
    "\n",
    "### Passthrough\n",
    "\n",
    "```yaml\n",
    "slices:\n",
    "  - sources:\n",
    "    - model: OpenPipe/mistral-ft-optimized-1218\n",
    "      layer_range: [0, 32]\n",
    "  - sources:\n",
    "    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "      layer_range: [24, 32]\n",
    "merge_method: passthrough\n",
    "dtype: bfloat16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cg123/mergekit.git\n",
    "!cd mergekit && pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "MODEL_NAME = \"Marcoro14-7B-slerp_v2\"\n",
    "yaml_config = \"\"\"\n",
    "slices:\n",
    "  - sources:\n",
    "      - model: OpenPipe/mistral-ft-optimized-1218\n",
    "        layer_range: [0, 32]\n",
    "      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "        layer_range: [0, 32]\n",
    "merge_method: slerp\n",
    "base_model: OpenPipe/mistral-ft-optimized-1218\n",
    "parameters:\n",
    "  t:\n",
    "    - filter: self_attn\n",
    "      value: [0, 0.5, 0.3, 0.7, 1]\n",
    "    - filter: mlp\n",
    "      value: [1, 0.5, 0.7, 0.3, 0]\n",
    "    - value: 0.5\n",
    "dtype: bfloat16\n",
    "\"\"\"\n",
    "\n",
    "with open('config.yaml', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Models\n",
    "!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU huggingface_hub\n",
    "\n",
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "from jinja2 import Template\n",
    "\n",
    "usernmae = \"wonik-hi\"\n",
    "\n",
    "template_text = \"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- merge\n",
    "- mergekit\n",
    "- lazymergekit\n",
    "{%- for model in models %}\n",
    "- {{ model }}\n",
    "{%- endfor %}\n",
    "---\n",
    "\n",
    "# {{ model_name }}\n",
    "\n",
    "{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
    "\n",
    "{%- for model in models %}\n",
    "* [{{ model }}](https://huggingface.co/{{ model }})\n",
    "{%- endfor %}\n",
    "\n",
    "## ğŸ§© Configuration\n",
    "\n",
    "```yaml\n",
    "{{- yaml_config -}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())\n",
    "\n",
    "# Get list of models from config\n",
    "data = yaml.safe_load(yaml_config)\n",
    "if \"models\" in data:\n",
    "    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
    "elif \"parameters\" in data:\n",
    "    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
    "elif \"slices\" in data:\n",
    "    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
    "else:\n",
    "    raise Exception(\"No models or slices found in yaml config\")\n",
    "\n",
    "\n",
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "    model_name=MODEL_NAME,\n",
    "    models=models,\n",
    "    yaml_config=yaml_config,\n",
    "    username=username,\n",
    ")\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save('merge/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{username}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.coalb import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"wonik-hi\"\n",
    "\n",
    "api = HfAPi(token=userdata.get(\"HF_API_KEY2\"))\n",
    "\n",
    "\"\"\"\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username}/MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\"\"\"\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIES Merge ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "MODEL_NAME = \"wonik-hi_merge_model\"\n",
    "yaml_config = \"\"\"\n",
    "models:\n",
    "  - model: wonik-hi/Marcoro14-7B-slerp\n",
    "    parameters:\n",
    "      density: [1, 0.7, 0.1] # density gradient\n",
    "      weight: 1.0\n",
    "  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
    "    parameters:\n",
    "      density: 0.33\n",
    "      weight:\n",
    "        - filter: mlp\n",
    "          value: 0.5\n",
    "        - value: 0\n",
    "merge_method: ties\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "parameters:\n",
    "  normalize: true\n",
    "  int8_mask: true\n",
    "dtype: float16\n",
    "name: gradient-slerp-ties\n",
    "\"\"\"\n",
    "\n",
    "# Save config as yaml file\n",
    "with open('/content/conf.yml', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ìˆ˜ ì…‹íŒ…\n",
    "OUTPUT_PATH = \"./merged_v2\" # ì €ì¥ë  ê²½ë¡œ\n",
    "LORA_MERGE_CACHE = \"/tmp\" # ë¡œë¼ ë³‘í•© ìºì‹œ ì €ì¥ ê²½ë¡œ\n",
    "CONFIG_YML = \"/content/conf.yml\" # config yml íŒŒì¼ ê²½ë¡œ\n",
    "COPY_TOKENIZER = True\n",
    "LAZY_UNPICKLE = True # ì‹¤í—˜ì ì¸ ì €ìš©ëŸ‰ ëª¨ë¸ ë¡œë” ê¸°ëŠ¥ í™œì„±í™”\n",
    "LOW_CPU_MEMORY = True # ì„œë²„ ì„±ëŠ¥ ì¢‹ìœ¼ë©´ True ì„¤ì •\n",
    "ALLOW_CRIMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cg123/mergekit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd mergekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¹ƒì— ì…‹íŒ…ëœ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "# yml íŒŒì¼ ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(), # GPU ì„¸íŒ…\n",
    "        copy_tokenizer=COPY_TOKENIZER, # í† í¬ë‚˜ì´ì €\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "        allow_crimes=ALLOW_CRIMES,\n",
    "    ),\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"wonik-hi\"\n",
    "\n",
    "api = HfApi(token=userdata.get(\"huggingface\"))\n",
    "\n",
    "\"\"\"\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"merged_v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- ì‹¤í–‰í™˜ê²½ : T4 ì •ë„ì˜ GPU ì„±ëŠ¥ í•„ìš”\n",
    "- KoBEST : Korean Balanced Evaluation of Singificant Tasks\n",
    "ì˜ êµ¬ì„±ëœ ë²¤ì¹˜ë§ˆí¬ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê°ê´€ì ì´ê³  ì •ë°€í•˜ê²Œ í‰ê°€ -> ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ë°œì „ì„ ì´‰ì§„\n",
    "\n",
    "- Ko LM Eval Harness : í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ì ìˆ˜\n",
    "    - í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— colabì—ì„œ ì§„í–‰í•˜ë©´ ëŠê¸¸ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŒ.\n",
    "    - ì‹¤í–‰ ì‹œ ë¡œì»¬ë¡œ ë‚´ë¦¬ê³  ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•¨.\n",
    "    - ëª¨ë“  í‰ê°€ë¥¼ í•œë²ˆì— ì§„í–‰í•˜ëŠ” ê²ƒì€ ë„ˆë¬´ ì˜¤ë˜ ê±°ë¦´ë¯€ë¡œ, íƒœìŠ¤í¬ë¥¼ ë‚˜ëˆ ì„œ ì§„í–‰í•˜ë„ë¡ í•˜ëŠ”ê²ƒì„ ê¶Œì¥.\n",
    "``` shell\n",
    "git clone https://github.com/Beomi/ko-lm-evalutation-harness\n",
    "cd ko-lm-evaluation-harness\n",
    "pip isntall -r requirements.txt\n",
    "./run_all.sh ëª¨ë¸ì´ë¦„ 'GPUë²ˆí˜¸ë“¤'\n",
    "\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # ì´ë ‡ê²Œ í•˜ë©´ GPU 0,1ë²ˆì— 'eomi/llama-2-ko-7b' ëª¨ë¸ì„ ìª¼ê°œì„œ ì˜¬ë¦¬ê³  í‰ê°€í•œë‹¤.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Ko BEST Dataset\n",
    "huggingface.co/datasets/skt/kobest_v1/viewer/boolq/train\n",
    "\n",
    "\n",
    "### Ko BEST Eval Code\n",
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/new_mistral7B,use_accelerate=true, trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/new_mistral7B/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Beomi/ko-lm-evaluation-harness\n",
    "%cd ko-lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "#./run_all.sh ëª¨ë¸ì´ë¦„ 'GPUë²ˆí˜¸ë“¤'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜í–‰ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼ ì£¼ì˜!!!\n",
    "! ./run_all.sh 'wonik-hi/phi-3-mini-QLoRA' '0'\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # ì´ë ‡ê²Œ í•˜ë©´ GPU0,1ë²ˆì— 'beomi/llama-2-ko-7b' ëª¨ë¸ì„ ìª¼ê°œì„œ ì˜¬ë¦¬ê³  í‰ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜í–‰ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼ ì£¼ì˜!!!\n",
    "# ./run_all.sh model GPU_NO few-shot test\n",
    "! ./run_all.sh 'oz1115/phi-3-mini-QLoRA' '0' \"0\" 0.2\n",
    "# ex) ./run_all.sh beomi/llama-2-ko-7b '0,1' # ì´ë ‡ê²Œ í•˜ë©´ GPU0,1ë²ˆì— 'beomi/llama-2-ko-7b' ëª¨ë¸ì„ ìª¼ê°œì„œ ì˜¬ë¦¬ê³  í‰ê°€í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixtral eval\n",
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/new_mistral7B,use_accelerate=true,trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/new_mistral7B/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained=wonik-hi/phi-3-mini-QLoRA,use_accelerate=true,trust_remote_code=true \\\n",
    "--tasks kobest_hellaswag \\\n",
    "--num_fewshot 0 \\\n",
    "--no_cache \\\n",
    "--batch_size 8 \\\n",
    "--output_path result/all/wonik-hi/phi-3-mini-QLoRA2/0_shot_kobest_hellaswag.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# results/korean_origin_bench/20b/00_shot.json\n",
    "def _get_metric_name(v):\n",
    "    metrics = ['f1', 'macro_f1', 'acc_norm', 'acc']\n",
    "    for m in metrics:\n",
    "        if v.get(m):\n",
    "            return {\n",
    "                'metric': m,\n",
    "                'value': v[m],\n",
    "            }\n",
    "\n",
    "def get_df_klue(path, model_name=''):\n",
    "    data = []\n",
    "    for i in ['0', '5', '10', '50']:\n",
    "        shot = f'{path}/{i}_shot.json'\n",
    "        try:\n",
    "            data.append(\n",
    "                {\n",
    "                    f\"{k} ({_get_metric_name(v)['metric']})\": _get_metric_name(v)['value']\n",
    "                    for k, v in json.load(open(shot))['results'].items()\n",
    "                }\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    df = pd.DataFrame(data, index=[0, 5, 10, 50][:len(data)]).T\n",
    "    print(df.to_markdown())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_klue('results/klue_etc_bench/home/jovyan/beomi/llama2-koen-13b/60b', 'llama2-koen-13b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_models = sorted(glob('results/all/*/*'))\n",
    "various_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in various_models:\n",
    "    print(model)\n",
    "    get_df_klue(model)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
