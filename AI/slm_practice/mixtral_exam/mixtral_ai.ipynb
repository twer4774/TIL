{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtral AI\n",
    "- Mixtral 8 * 7B : Llama2에 비해 6배 빠름. 코드 생성에 강점.\n",
    "    - 전문화된 모델이 8개가 7B 정도가 각각 있는 형태이고, 공통 파라미터를 제외하면 47B정도가 됨.\n",
    "\n",
    "## MoE(Mixture of Experts)\n",
    "- 여러 전문가 네트워크를 분할함.\n",
    "- 입력에 대해 적합한 전문가 모델 선택해서 실행.\n",
    "- 정의\n",
    "    - Tow main elements\n",
    "        - Spare MoE layers : 계층적 MoE\n",
    "        - A gate network or router\n",
    "    - Challenges\n",
    "        - Training: 효율적인 pretraining 가능. 단점 : overfitting \n",
    "        - Inference: 동일한 숫자의 파라미터를 가진 다른 모델 보다 추론 속도는 빠름. 단점 : 메모리에 모든 모델이 올라가야하므로 높은 메모리 성능 필요\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
