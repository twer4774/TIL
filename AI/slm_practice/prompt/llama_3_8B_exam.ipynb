{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.30.1\n",
    "!pip install transformers==4.39.3\n",
    "!pip install gradio==4.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Specify the model ID for the pre-trained model from Hugging Face\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer using the specified model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Configure the BitsAndBytes settings for loading the model\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the model for causal language modeling using the specified model ID\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 as the data type for the model\n",
    "    device_map=\"auto\",          # device_map=\"auto\" automatically maps the model to available devices (like GPU if available)\n",
    "    quantization_config=config  # quantization_config=config specifies the quantization configuration for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutli Turn Prompt Engineering\n",
    "- 여러 차례의 대화 턴(turn)을 통해 LLM에 정보를 제공하고,\n",
    "이를 바탕으로 더욱 정교한 응답을 생성하도록 하는 Prompt Engineering 기법\n",
    "- 자연스러운 대화 생성 가능 : Chatbot에서 대부분 지원되어야 함.\n",
    "- 문맥 이해 강화 : 한 번의 턴 만을 고려하는 대신, 여러 턴의 대화를 고려하면 모델이 문맥을 더 잘 이해함\n",
    "- 예시\n",
    "    ```\n",
    "    Input\n",
    "    질문 : 미국 수도가 어디야??\n",
    "    대답 : 미국의 수도는 워싱턴 D.C. 입니다.\n",
    "    질문 : 그럼 한국은?\n",
    "\n",
    "    Output\n",
    "    대답 : 한국의 수도는 서울입니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a nice chatbot that helps users. You always have to respond briefly, within three sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of the United States?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of the United States is Washington D.C.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Then, what about Korea?\"}\n",
    "]\n",
    "\n",
    "# 토큰나이즈\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # 다음 추론을 자동으로 넣어는 옵션\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "# Terminators 설정\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 토큰을 제외하고 응답 토큰 출력\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "# string으로 변환\n",
    "print(\"repsonse : \", tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompt Engineering\n",
    "- LLM에 예제를 제공하지 않고 응답을 생성하게 하는 기법\n",
    "- LLM 성능 평가 시, 보통 사용되는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial conversation messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Korea robot that summarizes documents. You MUST answer in Korea\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        ###document: 기후 변화는 수십 년에서 수백만 년에 걸친 기간 동안의 기상 패턴의 통계적 분포에서 장기적인 변화를 의미합니다.\n",
    "        이는 평균 기상 조건의 변화, 또는 평균 조건 주변의 기상 분포의 변화를 의미할 수 있습니다.\n",
    "        또한 이것은 기온, 강수량, 또는 바람 패턴의 변화를 포함할 수 있습니다.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Remove newline characters from the content of each message\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,                    # List of conversation messages\n",
    "    add_generation_prompt=True,  # Add a text generation prompt\n",
    "    return_tensors=\"pt\"          # Return the results as PyTorch tensors\n",
    ").to(model.device)               # Move\n",
    "\n",
    "# Define a list of terminators, which specifies the end-of-sequence token ID\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model outputs based on the input IDs\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Extract the generated response tokens, excluding the input prompt tokens\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "# Decode the response tokens back into a string, skipping special tokens\n",
    "print(\"response : \", tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompt Engineering\n",
    "- LLM에 몇 가지(Few) 예제를 제공하여 응답을 생성하게 하는 기법\n",
    "- Zero-Shot 보다 더 나은 성능을 보임\n",
    "- 예제 개수에 따라 , 1-shot, 3-shot 등으로 불림\n",
    "- 일정 크기 이상의 LLM에 적용 가능\n",
    "    ```\n",
    "    ex)\n",
    "    Input\n",
    "    질문 : 너는 리뷰의 긍/부정을 알려주는 로봇이야 \"오늘 시켰는데 정말 맛있었어요\"\n",
    "    대답 : \"긍정\"\n",
    "    질문 : \"별로 였어요\"\n",
    "    대답 : \"부정\"\n",
    "    질문 : \"한번 더 시켜먹고 싶어요\"\n",
    "\n",
    "    Output\n",
    "    대답 : \"긍정\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial conversation messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a korea robot that summarizes documents. You MUST answer in Korea\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    ###document: 에펠탑은 프랑스 파리의 샹 드 마르스에 위치한 철제 격자 탑입니다.\n",
    "    이 탑은 그것을 설계하고 건설한 회사의 엔지니어인 구스타브 에펠타의 이름을 딴 것입니다.\n",
    "    1887년부터 1889년까지 1889년 세계 박람회의 입구로 건설되었으며,\n",
    "    처음에는 그 디자인을 이유로 프랑스의 주요 예술가들과 지식인들로부터 비판을 받았습니다.\n",
    "    하지만 이제는 프랑스의 전세계적인 문화 아이콘 그리고 세계에서 가장 알아보기 쉬운 구조물 중 하나가 되었습니다.\n",
    "    \"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"에펠탑은 파리의 철제 격자 구조물로, 1887년부터 1889년까지 건설되었으며,\n",
    "    이후로는 프랑스의 전 세계적으로 인식된 상징이 되었습니다.\"\"\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    ###document: 애플은 컴퓨터 소프트웨어와 온라인 서비스를 설계, 개발, 판매하는 미국의 다국적 기술 회사로,\n",
    "     캘리포니아 주 쿠퍼티노에 본사를 두고 있습니다.\n",
    "     애플은 아마존, 구글, 마이크로소프트, 페이스북과 함께 빅 테크 기술 회사로 간주됩니다.\n",
    "    \"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"애플은 소비자 전자 제품과 소프트웨어를 설계하고 판매하는 주요 기술 회사로,\n",
    "    아마존, 구글, 마이크로소프트, 페이스북과 함께 빅 테크로 간주됩니다.\"\"\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    ###document: 기후 변화는 수십 년에서 수백만 년에 걸친 기간 동안의 기상 패턴의 통계적 분포에서 장기적인 변화를 의미합니다.\n",
    "    이는 평균 기상 조건의 변화, 또는 평균 조건 주변의 기상 분포의 변화를 의미할 수 있습니다.\n",
    "    또한 이것은 기온, 강수량, 또는 바람 패턴의 변화를 포함할 수 있습니다.\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "# Remove newline characters from the content of each message\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
