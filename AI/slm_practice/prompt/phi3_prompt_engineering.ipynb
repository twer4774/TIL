{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.30.1\n",
    "!pip install transformers==4.42.3\n",
    "!pip install gradio==4.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "# Specify the model ID for the pre-trained model from Hugging Face\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load the tokenizer using the specified model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model for causal language modeling using the specified model ID\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutli Turn Prompt Engineering\n",
    "- 여러 차례의 대화 턴(turn)을 통해 LLM에 정보를 제공하고,\n",
    "이를 바탕으로 더욱 정교한 응답을 생성하도록 하는 Prompt Engineering 기법\n",
    "- 자연스러운 대화 생성 가능 : Chatbot에서 대부분 지원되어야 함.\n",
    "- 문맥 이해 강화 : 한 번의 턴 만을 고려하는 대신, 여러 턴의 대화를 고려하면 모델이 문맥을 더 잘 이해함\n",
    "- 예시\n",
    "    ```\n",
    "    Input\n",
    "    질문 : 미국 수도가 어디야??\n",
    "    대답 : 미국의 수도는 워싱턴 D.C. 입니다.\n",
    "    질문 : 그럼 한국은?\n",
    "\n",
    "    Output\n",
    "    대답 : 한국의 수도는 서울입니다.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"HI What's your name?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"My name is joonhyung kim\"},\n",
    "    {\"role\": \"user\", \"content\": \"Would you like to say it again? What's your name?\"},\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model's output based on the input IDs\n",
    "outputs = model.generate(\n",
    "    input_ids,               # The input tensor containing the tokenized messages\n",
    "    max_new_tokens=1024,     # Maximum number of new tokens to generate in the response\n",
    "    do_sample=False,         # Disable sampling; use deterministic decoding (e.g., greedy or beam search)\n",
    "    # temperature=0.3,       # Temperature for sampling; lower values make the model more deterministic\n",
    "    # top_p=0.9,             # Top-p sampling; only keep the top tokens with cumulative probability <= top_p\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who do you like more, mom or dad?\"}\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    # temperature=0.3,\n",
    "    # top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Knowledge Prompting Engineering\n",
    "- LLM으로부터 먼저 지식을 생성하게 하도록 지시하고, 생성된 지식을 Prompt에 포함하여 답변을 생성하도록 하는 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is Greece larger than mexico?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is the Eiffel Tower taller than the Leaning Tower of Pisa?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : The Eiffel Tower is approximately 330 meters tall, while the Leaning Tower of Pisa is approximately 56 meters tall, making the Eiffel Tower considerably taller.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is the population of Canada greater than Australia?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : As of current data, the population of Canada is approximately 37 million, while the population of Australia is approximately 25 million, making Canada's population greater than Australia's.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Can you explain about Mother?\"}\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    # temperature=0.3,\n",
    "    # top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "Knowledge_1 = tokenizer.decode(response, skip_special_tokens=True)\n",
    "print(Knowledge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is Greece larger than mexico?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is the Eiffel Tower taller than the Leaning Tower of Pisa?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : The Eiffel Tower is approximately 330 meters tall, while the Leaning Tower of Pisa is approximately 56 meters tall, making the Eiffel Tower considerably taller.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Is the population of Canada greater than Australia?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge : As of current data, the population of Canada is approximately 37 million, while the population of Australia is approximately 25 million, making Canada's population greater than Australia's.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question : Can you explain about Father?\"}\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    # temperature=0.3,\n",
    "    # top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "Knowledge_2 = tokenizer.decode(response, skip_special_tokens=True)\n",
    "print(Knowledge_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2\n",
    "- Step1 응답 결과를 프롬프트에 다시 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    {Knowledge_1}{Knowledge_2}\n",
    "    Question : Given the Knowledge, Who do you like more, mom or dad?\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    # temperature=0.3,\n",
    "    # top_p=0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
