{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.30.1\n",
    "# !pip install transformers==4.39.3\n",
    "!pip install transformers==4.42.3\n",
    "!pip install gradio==4.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "# Specify the model ID for the pre-trained model from Hugging Face\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load the tokenizer using the specified model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Configure the BitsAndBytes settings for loading the model\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought(CoT) Prompt Engineering\n",
    "- LLM이 답변을 생성할 때, 단순히 최종 답만 생성하는 것이 아닌, 그 과정을 단계별로 나누어서 생각하도록 하는 기법\n",
    "- 추론 과정의 투명성 : LLM의 작동방식을 더 잘 이해할 수 있음\n",
    "- 복잡한 문제 해결 능력 : 여러 단계의 추론 과정을 효과적으로 모델링할 수 있음\n",
    "- 학습 효율성 향상 : 새로운 문제에 대해 빠르게 적응할 수 있게\n",
    "```\n",
    "Input\n",
    "질문 : 너는 리뷰의 긍부정을 알려주는 로봇이야\n",
    "    왜 그렇게 생각했는지 말해주고, 그에 따른 분류를 해주면 돼.\n",
    "    \"오늘 시켰는데 정말 맛있어요\"\n",
    "대답 : \"맛있어요\" 키워드가 있으므로, \"긍정\"\n",
    "질문 : \"별로 였어요 ㅠ\"\n",
    "대답 : \"별로\" 키워드가 있으므로, \"부정\"\n",
    "질문 : \"한번 더 시켜먹고 싶어요\"\n",
    "\n",
    "Output\n",
    "대답 : \"한번 더 시켜먹고\" 키워드가 있으므로 \"긍정\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT는 시스템 프롬프트가 따로 없음.\n",
    "# few-shot을 통해 응답값 조율\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me solve this quadratic equation: x^2 - 3x - 4 = 0?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "    Sure, to solve the quadratic equation x^2 - 3x - 4 = 0,\n",
    "    we can use the quadratic formula, x = [-b ± sqrt(b² - 4ac)] / (2a).\n",
    "    In this equation, a=1, b=-3, and c=-4.\n",
    "    First, let's calculate the value under the square root,\n",
    "    b² - 4ac, which is (-3)² - 4*1*(-4).\n",
    "    The value is 9 + 16, which equals to 25. Next, we need to compute the values of x using the quadratic formula.\n",
    "    So, x = [3 ± sqrt(25)] / 2. The roots of the equation are x = 4 and x = -1.\n",
    "    \"\"\"\n",
    "    },\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me solve this quadratic equation: x^2 - 6x + 9 = 0?\"}\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message[\"content\"] = message[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Prompt Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤리적 필터링\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"\"\"You're given a list of moderation categories as below:\n",
    "- illegal: Illegal activity.\n",
    "- child abuse: child sexual abuse material or any content that exploits or harms children.\n",
    "- hate violence harassment: Generation of hateful, harassing, or violent content: content that expresses, incites, or promotes hate based on identity, content that intends to harass, threaten, or bully an individual, content that promotes or glorifies violence or celebrates the suffering or humiliation of others.\n",
    "- malware: Generation of malware: content that attempts to generate code that is designed to disrupt, damage, or gain unauthorized access to a computer system.\n",
    "- physical harm: activity that has high risk of physical harm, including: weapons development, military and warfare, management or operation of critical infrastructure in energy, transportation, and water, content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.\n",
    "- economic harm: activity that has high risk of economic harm, including: multi-level marketing, gambling, payday lending, automated determinations of eligibility for credit, employment, educational institutions, or public assistance services.\n",
    "- fraud: Fraudulent or deceptive activity, including: scams, coordinated inauthentic behavior, plagiarism, academic dishonesty, astroturfing, such as fake grassroots support or fake review generation, disinformation, spam, pseudo-pharmaceuticals.\n",
    "- adult: Adult content, adult industries, and dating apps, including: content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness), erotic chat, pornography.\n",
    "- political: Political campaigning or lobbying, by: generating high volumes of campaign materials, generating campaign materials personalized to or targeted at specific demographics, building conversational or interactive systems such as chatbots that provide information about campaigns or engage in political advocacy or lobbying, building products for political campaigning or lobbying purposes.\n",
    "- privacy: Activity that violates people's privacy, including: tracking or monitoring an individual without their consent, facial recognition of private individuals, classifying individuals based on protected characteristics, using biometrics for identification or assessment, unlawful collection or disclosure of personal identifiable information or educational, financial, or other protected records.\n",
    "- unqualified law: Engaging in the unauthorized practice of law, or offering tailored legal advice without a qualified person reviewing the information.\n",
    "- unqualified financial: Offering tailored financial advice without a qualified person reviewing the information.\n",
    "- unqualified health: Telling someone that they have or do not have a certain health condition, or providing instructions on how to cure or treat a health condition.\n",
    "Please classify the following text into one of these categories, and answer with that single word only.\n",
    "If the sentence does not fall within these categories, is safe and does not need to be moderated, please answer \"not moderated\".\n",
    "Text: 개인정보는 무조건 팔아야지\"\"\"\n",
    "}]\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
