{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.30.1\n",
    "!pip install transformers==4.39.3\n",
    "!pip install gradio==4.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "model_id = \"google/gemma-1.1-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompt Engineering\n",
    "- LLM에 예제를 제공하지 않고 응답을 생성하게 하는 기법\n",
    "- LLM 성능 평가 시, 보통 ㅏㅅ용되는 기법\n",
    "\n",
    "# Few-Shot Prompt Engineering\n",
    "- LLM에 몇 가지(Few) 예제를 제공하여 응답을 생성하게 하는 기법\n",
    "- Zero-Shot 보다 더 나은 성능을 보임\n",
    "- 예제 개수에 따라 , 1-shot, 3-shot 등으로 불림\n",
    "- 일정 크기 이상의 LLM에 적용 가능\n",
    "    ```\n",
    "    ex)\n",
    "    Input\n",
    "    질문 : 너는 리뷰의 긍/부정을 알려주는 로봇이야 \"오늘 시켰는데 정말 맛있었어요\"\n",
    "    대답 : \"긍정\"\n",
    "    질문 : \"별로 였어요\"\n",
    "    대답 : \"부정\"\n",
    "    질문 : \"한번 더 시켜먹고 싶어요\"\n",
    "\n",
    "    Output\n",
    "    대답 : \"긍정\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\": \"You are a robot that tells whether a review comment is positive or negative. \\nreview comment : 다신 안먹어요\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : \"negative\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"review comment : 맨날 먹어요 ㅎㅎ\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : \"positive\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"review comment : 가격이 2배되도 시켜먹겠습니다.\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Ask Prompt Enginerring\n",
    "- LLM이 스스로 질문을 던지며 문제 해결을 하면서, 최종적으로 답변을 생성하게 하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": \"\"\"\n",
    "    You are an English Teacher who teaches Korean Students.\n",
    "    You always have to explain in the format of a conversation between a student and teacher.\n",
    "    sententce : 나는 아버지가 방에 들어가는 모습을 보고 많이 후회하고 힘들어했다.\n",
    "    \"\"\" },\n",
    "    {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "    Teacher: What is the verb in the sentence?\n",
    "    Student: The verb is '후회하고 힘들어했다' which translates to 'regretted and struggled'.\n",
    "    Teacher: What is the object of the sentence?\n",
    "    Student: The object is '아버지가 방에 들어가는 모습' which translates to 'the sight of my father entering the room'.\n",
    "    Teacher: Now, can you try to put it all together in English?\n",
    "    Student: Yes, the sentence in English would be, \"I regretted and struggled a lot after seeing my father entering the room.\"\n",
    "    \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"sentence : 어제 밤에 일이 너무 힘들어서 나는 새벽에 깨서 엉엉 울었다.\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Chaining\n",
    "- 예시 프롬프트를 제공하고 => 학습\n",
    "- context를 넣어서 질문, 답변을 생성하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 예시 프롬프트 제공\n",
    "few_shot_context = \"LG전자가 임직원들에게 무료로 사내식당 조식을 제공키로 했다.업계에 따르면 LG전자는 내달 1일부터 3만5000여 명에 달하는 국내 전 사업장 임직원들..\"\n",
    "few_shot_question = \"LG전자의 국내 전 사업자 임직원들을 몇 명인가요?\"\n",
    "few_shot_answer = \"LG전자의 국내 전 사업자 임직원 수는 약 3만 5000명입니다.\"\n",
    "\n",
    "# 질문과 답변을 생성하도록 함\n",
    "context = \"예산군은 2024년도 여름방학 대학생 아르바이트 희망자 40명을 6월 24일부터 26일까지 모집한다고 밝혔다.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : f\"\"\"You are a robot that generates question and answers using the given context. \\n You MUST generate in Korean with JSON. \\ncontext : {few_shot_context}\"\"\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{{\\\"question\\\" : \\\"{few_shot_question}\\\", \\\"answer\\\" : \\\"{few_shot_answer}\\\"}}\"},\n",
    "    {\"role\" : \"user\", \"content\" : f\"context : {context}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# '{\"question\": \"예산군이 2024년도 여름방학 대학생 아르바이트 희망자를 언제 모집할까요?\", \"answer\": \"예산군은 2024년도 여름방학 대학생 아르바이트 희망자를 6월 24일부터 26일까지 모집합니다.\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = json.loads(tokenizer.decode(response, skip_special_tokens=True))['question']\n",
    "answer= json.loads(tokenizer.decode(response, skip_special_tokens=True))['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question\n",
    "# '예산군이 2024년도 여름방학 대학생 아르바이트 희망자를 언제 모집할까요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"예산군은 2024년도 여름방학 대학생 아르바이트 희망자를 6월 24일부터 25일까지 모집합니다.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context\n",
    "# '예산군은 2024년도 여름방학 대학생 아르바이트 희망자 40명을 6월 24일부터 26일까지 모집한다고 밝혔다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\" :f\"\"\"You are a robot that judges whether the answer is correct by looking at the content, question, and answer. Judge and explain why you think so in Korean.\n",
    "    \\ncontext : {few_shot_context}\\nquestion : {few_shot_question}\\nanswer : {few_shot_answer}\"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\" : \"\"\"{{\"result\" : \"True\", \"reason\" : \"context에 35000명이라고 명시가 되어 있기 때문입니다\"}}\"\"\"},\n",
    "    {\"role\": \"user\", \"content\" : f\"context : {context}\\nquestion : {question}\\nanswer : {answer}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# '{{\"result\" : \"False\", \"reason\" : \"문제에서 제공된 답은 \\'6월 24일부터 26일까지\\'입니다. 하지만 질문은 \\' 언제\\'라고 물고 있습니다.\"}}'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
