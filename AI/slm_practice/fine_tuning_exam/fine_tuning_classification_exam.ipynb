{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미국 뉴스 데이터에서 야구, 하키 뉴스 가져오기\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "\n",
    "# subset : 데이터 훈련용/테스트용/전체 선택 가능\n",
    "# suffle : 데이터를 로드할때 섞을지 여부 지정\n",
    "# random_state : 데이터 셔플링 시 랜덤 시드 설정\n",
    "# categories : 로드할 뉴스 그룹 카테고리 지정\n",
    "sports_dataset = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories)\n",
    "\n",
    "# 다운로드 받은 데이터 일부 확인\n",
    "print(sports_dataset['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "- 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_dataset.keys()\n",
    "# sports_dataset\n",
    "sports_dataset.target_names[sports_dataset['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all, len_baseball, len_hockey = len(sports_dataset.data), len([e for e in sports_dataset.target if e == 0]), len([e for e in sports_dataset.target if e == 1])\n",
    "print(f\"Total examples: {len_all}, Baseball examples: {len_baseball}, Hockey examples: {len_hockey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- 데이터 전처리 : 불필요한 데이터 제거 & 텍스트만 추출\n",
    "- Pandas를 이용해 prompt와 completion으로 정제함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = [sports_dataset.target_names[x].split('.')[-1] for x in sports_dataset['target']] # rec.sport.baseball --> baseball\n",
    "texts = [text.strip() for text in sports_dataset['data']]                                   #data 중, text만 추려냄.\n",
    "df = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"sport2.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation tool\n",
    "- 텍스트 데이터를 GPT 모델을 위한 파인 튜닝용 형식으로 준비하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 함수는 데이터를 특정 형식으로 변환하고, 모델이 학습할 수 있도록 전처리\n",
    "# OpenAI에서 제공하는 tool의 기능으로, fine_tuning_exam.ipynb에서 수행했던 싱글쿼터('), 불필요한 문자열 제거를 자동 수행해준다.\n",
    "!openai tools fine_tunes.prepare_data -f sport2.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install token-count\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 카운트\n",
    "# 주어진 json lines 파일에서 특정 수의 토큰 이상을 포함하지 않도록 데이터를 조정한후\n",
    "# 조정한 데이터를 새로운 파일에 저장\n",
    "# 이 함수는 텍스트 데이터를 특정 토큰 제한 내에서 처리하는데 유용\n",
    "# 파일의 'prompt'와 'completetion' 필드의 토큰 수를 계산하고, 특정 토큰 수를 초과하지 않도록 조정. 조정후 새 파일에 저장\n",
    "from token_count import TokenCount\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "def token_limit(file_name, update_file_name, token_cnt):\n",
    "  tc = TokenCount(model_name='gpt-3.5-turbo')\n",
    "  #print(tc.num_tokens_from_file(file_name))\n",
    "\n",
    "  tot_tokens=0\n",
    "  update_txt =[]\n",
    "  tmp1 =\"\"\n",
    "  tmp_cnt= 0\n",
    "\n",
    "  with jsonlines.open(file_name) as f:\n",
    "    for line in f.iter():\n",
    "      tmp1 = \"\"\n",
    "      tmp_cnt = 0\n",
    "      for token in line[\"prompt\"].split(' '):\n",
    "        tmp1 += token\n",
    "        tmp_cnt += 1\n",
    "\n",
    "        if tmp_cnt > 500:\n",
    "          break\n",
    "      line[\"prompt\"]=tmp1\n",
    "      #print(line[\"prompt\"])\n",
    "\n",
    "      tokens = tc.num_tokens_from_string(line[\"prompt\"])\n",
    "      tokens2 = tc.num_tokens_from_string(line[\"completion\"])\n",
    "      tot_tokens += tokens + tokens2\n",
    "      if tot_tokens > token_cnt:\n",
    "        break\n",
    " #     print(tot_tokens)\n",
    "      update_txt.append(line)\n",
    "\n",
    "#  print(tot_tokens)\n",
    "#  print(update_txt)\n",
    "\n",
    "  with open(update_file_name, 'w', encoding=\"utf-8\") as f:\n",
    "    for item in update_txt:\n",
    "      json.dump(item, f, ensure_ascii=False)\n",
    "      f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 업데이트  함수 추가\n",
    "import json\n",
    "\n",
    "def update_data(file, update_file):\n",
    "  data_list = []\n",
    "# 기존 프롬프트-완료 데이터 로드\n",
    "  data_list = []\n",
    "  with open(file, \"r\") as f:\n",
    "      for line in f:\n",
    "          data_list.append( json.loads(line) )\n",
    "\n",
    "  # 채팅 형식으로 변환\n",
    "  chat_data = []\n",
    "  for entry in data_list:\n",
    "      chat_entry = {\n",
    "          \"messages\": [\n",
    "              {\"role\": \"system\", \"content\": \"당신은 친절한 비서입니다.\"},  # 시스템 메시지는 필요에 따라 변경 가능\n",
    "              {\"role\": \"user\", \"content\": entry[\"prompt\"]},\n",
    "              {\"role\": \"assistant\", \"content\": entry[\"completion\"].strip()}\n",
    "          ]\n",
    "      }\n",
    "      chat_data.append(chat_entry)\n",
    "\n",
    "  str=\"\"\n",
    "  for item in chat_data:\n",
    "    str += json.dumps(item) + \"\\n\"\n",
    "\n",
    "  # 변환된 데이터를 새 파일로 저장\n",
    "  with open(update_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in chat_data:\n",
    "      json.dump(i, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장\n",
    "      f.write(\"\\n\") # json을 쓰는 것과 같지만, 여러 줄을 써주는 것이므로 \"\\n\"을 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_limit(\"sport2_prepared_train.jsonl\", \"sport2_prepared_train_drop.jsonl\", 4000)\n",
    "token_limit(\"sport2_prepared_valid.jsonl\", \"sport2_prepared_valid_drop.jsonl\", 4000)\n",
    "\n",
    "update_data(\"sport2_prepared_train_drop.jsonl\", \"sport2_prepared_train_drop.jsonl\")                   # 업데이트 함수 추가\n",
    "update_data(\"sport2_prepared_valid_drop.jsonl\", \"sport2_prepared_valid_drop.jsonl\")                   # 업데이트 함수 추가\n",
    "\n",
    "train_file = client.files.create(file=open(\"sport2_prepared_train_drop.jsonl\", \"rb\"), purpose=\"fine-tune\") # 트레이닝 파일\n",
    "valid_file = client.files.create(file=open(\"sport2_prepared_valid_drop.jsonl\", \"rb\"), purpose=\"fine-tune\") # 결과물을 체크하는 파일\n",
    "\n",
    "# 시간이 꽤 걸린다\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(training_file=train_file.id, validation_file=valid_file.id, model=\"gpt-4o-2024-08-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fine_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파인 튜닝 작업이 세부 정보를 가져옴\n",
    "fine_tune_results = client.fine_tuning.jobs.retrieve(fine_tuning_job.id).result_files\n",
    "result_file = client.files.retrieve(fine_tune_results[0])\n",
    "content = client.files.content(result_file.id)\n",
    "\n",
    "with open(\"result.csv\", \"wb\") as f:\n",
    "    f.write(content.text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 result_file에 저장된 값을 테이블 형식으로 변경해서 저장 - 사람이 볼 수 있도록 인코딩\n",
    "import base64\n",
    "base64.b64decode(content.text.encode(\"utf-8\"))\n",
    "\n",
    "# To save contet to file\n",
    "with open(\"result.csv\", \"wb\") as f:\n",
    "  f.write(base64.b64decode(content.text.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy  \\\n",
      "0      1     7.30027            0.25     9.26150                       0.25   \n",
      "1      2    10.73621            0.25     9.44178                       0.50   \n",
      "2      3     9.56514            0.25    10.81888                       0.25   \n",
      "3      4     8.66793            0.25     8.52589                       0.25   \n",
      "4      5     7.14792            0.25     7.48306                       0.25   \n",
      "..   ...         ...             ...         ...                        ...   \n",
      "94    95     0.00104            1.00     0.00040                       1.00   \n",
      "95    96     0.00010            1.00     0.00039                       1.00   \n",
      "96    97     0.00031            1.00     0.00021                       1.00   \n",
      "97    98     0.00018            1.00     0.00011                       1.00   \n",
      "98    99     0.00066            1.00     0.00011                       1.00   \n",
      "\n",
      "    train_mean_reward  full_validation_mean_reward  \n",
      "0                 NaN                          NaN  \n",
      "1                 NaN                          NaN  \n",
      "2                 NaN                          NaN  \n",
      "3                 NaN                          NaN  \n",
      "4                 NaN                          NaN  \n",
      "..                ...                          ...  \n",
      "94                NaN                          NaN  \n",
      "95                NaN                          NaN  \n",
      "96                NaN                          NaN  \n",
      "97                NaN                          NaN  \n",
      "98                NaN                          NaN  \n",
      "\n",
      "[99 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv('result.csv')\n",
    "results[results['train_accuracy'].notnull()].tail(1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 및 딥러닝 모델을 훈련할 때, 훈련 데이터셋이 모델의 예측 정확도를 나타내는 지표\n",
    "# 얼마나 잘 맞는지 평가하는데 사용\n",
    "results[results['train_accuracy'].notnull()]['train_accuracy'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 사용\n",
    "- 파인 튜닝 된 모델을 이용해 분류를 잘 하는지 확인 (baseball, hockey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': '당신은 친절한 비서입니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': '당신은 친절한 비서입니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': '당신은 친절한 비서입니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': '당신은 친절한 비서입니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': '당신은 친절한 비서입니다....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages\n",
       "0  [{'role': 'system', 'content': '당신은 친절한 비서입니다....\n",
       "1  [{'role': 'system', 'content': '당신은 친절한 비서입니다....\n",
       "2  [{'role': 'system', 'content': '당신은 친절한 비서입니다....\n",
       "3  [{'role': 'system', 'content': '당신은 친절한 비서입니다....\n",
       "4  [{'role': 'system', 'content': '당신은 친절한 비서입니다...."
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_json('sport2_prepared_valid_drop.jsonl', lines=True)\n",
    "# test = pd.read_json('sport2_prepared_valid.jsonl', lines=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hockey\n"
     ]
    }
   ],
   "source": [
    "ft_model = client.fine_tuning.jobs.retrieve('ftjob-IVfGEWOsM47hBGCcGQYcm5Cg').fine_tuned_model\n",
    "\n",
    "# gpt-3.5 방식\n",
    "# res = client.completions.create(model=ft_model, prompt=test['prompt'][5] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0)\n",
    "# gpt-4o 방식\n",
    "messages = test.loc[0, 'messages'] # 첫 번째 행의 'messages' 필드 가져오기\n",
    "user_message = next((msg['content'] for msg in messages if msg['role'] == 'user'), \"\")\n",
    "res = client.chat.completions.create(\n",
    "    model=ft_model, \n",
    "     messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message + '\\n\\n###\\n\\n'}\n",
    "    ], \n",
    "    max_tokens=10, \n",
    "    temperature=0)\n",
    "\n",
    "# res.choices[0].text # gpt-3.5 방식\n",
    "response_text = res.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "# OpenAI API를 통해 텍스트를 완성하는데 사용하는 함수\n",
    "# model:사용할 모델 지정, prompt: 프롬프트 지정, max_tokens: 생성할 최대 토큰 수, temperature: 생성 텍스트의 창의성 조정, n: 생성할 완성의 수, stop: 텍스트 생성을 멈출 문자열, logprobs: 모델이 예측한 각 토큰의 로그 확률을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 범용화\n",
    "- 모델에 다른 프롬프트를 입력해서 동작하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hockey\n"
     ]
    }
   ],
   "source": [
    "# teweet에 있는 내용으로 테스트\n",
    "sample_hockey_tweet = \"\"\"Thank you to the\n",
    "@Canes\n",
    " and all you amazing Caniacs that have been so supportive! You guys are some of the best fans in the NHL without a doubt! Really excited to start this new chapter in my career with the\n",
    "@DetroitRedWings\n",
    " !!\"\"\"\n",
    "# res = client.completions.create(model=ft_model, prompt=sample_hockey_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\n",
    "# res.choices[0].text\n",
    "\n",
    "# GPT-4 채팅 모델을 사용하여 요청\n",
    "res = client.chat.completions.create(\n",
    "    model=ft_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": sample_hockey_tweet + '\\n\\n###\\n\\n'}\n",
    "    ],\n",
    "    max_tokens=10,  # 토큰 수를 적절하게 설정\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 응답 내용 확인\n",
    "response_text = res.choices[0].message.content\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball\n"
     ]
    }
   ],
   "source": [
    "sample_baseball_tweet=\"\"\"BREAKING: The Tampa Bay Rays are finalizing a deal to acquire slugger Nelson Cruz from the Minnesota Twins, sources tell ESPN.\"\"\"\n",
    "# res = client.completions.create(model=ft_model, prompt=sample_baseball_tweet + '\\n\\n###\\n\\n', max_tokens=1, temperature=0, logprobs=2)\n",
    "# res.choices[0].text\n",
    "\n",
    "# GPT-4 채팅 모델을 사용하여 요청\n",
    "res = client.chat.completions.create(\n",
    "    model=ft_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": sample_baseball_tweet + '\\n\\n###\\n\\n'}\n",
    "    ],\n",
    "    max_tokens=10,  # 토큰 수를 적절하게 설정\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 응답 내용 확인\n",
    "response_text = res.choices[0].message.content\n",
    "print(response_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
