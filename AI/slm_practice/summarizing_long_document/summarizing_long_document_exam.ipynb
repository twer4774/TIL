{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 긴 문서 요약 방법 실습\n",
    "- 문장이 너무 요약되면 기존의 의미전달이 안되므로 적절한 방벙을 제시함\n",
    "- 어떠한 사이즈의 블럭단위로 자를 수 있는지 확인 가능\n",
    "    - 블럭단위로 문장을 요약한 후 블럭을 합치는 작업 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (1.58.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken # 토큰수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import driver\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"content/drive/MyDrive/gdrive/data/artificial_intelligence_wikipedia.txt\", \"r\") as file:\n",
    "with open(\"./artificial_intelligence_wikipedia.txt\", \"r\") as file:\n",
    "    artificial_intelligence_wikipedia_text = file.read()\n",
    "# print(artificial_inteliigence_wikipedia_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14630"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n",
    "len(encoding.encode(artificial_intelligence_wikipedia_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def get_chat_completion(messages, model='gpt-4-turbo'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n",
    "    return encoding.encode(text)\n",
    "\n",
    "\n",
    "def chunk_on_delimiter(input_string: str,\n",
    "                       max_tokens: int, delimiter: str) -> List[str]:\n",
    "    chunks = input_string.split(delimiter)\n",
    "    combined_chunks, _, dropped_chunk_count = combine_chunks_with_no_minimum(\n",
    "        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n",
    "    )\n",
    "    if dropped_chunk_count > 0:\n",
    "        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n",
    "\n",
    "    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n",
    "    return combined_chunks\n",
    "\n",
    "# 주어진 청크리스트를 받아서 각 조각을 합침 & 블럭 조합\n",
    "def combine_chunks_with_no_minimum(\n",
    "        chunks: List[str],\n",
    "        max_tokens: int,\n",
    "        chunk_delimiter=\"\\n\\n\",\n",
    "        header: Optional[str] = None,\n",
    "        add_ellipsis_for_overflow=False,\n",
    ") -> Tuple[List[str], List[int]]:\n",
    "    dropped_chunk_count = 0\n",
    "    output = []\n",
    "    output_indices = [] # 인덱스 저장\n",
    "    candidate = (\n",
    "        [] if header is None else [header]\n",
    "    )\n",
    "    candidate_indices = []\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        chunk_with_header = [chunk] if header is None else [header, chunk]\n",
    "        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n",
    "            print(f\"warning: chunk oveflow\")\n",
    "            if (\n",
    "                add_ellipsis_for_overflow\n",
    "                and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n",
    "            ):\n",
    "                candidate.append(\"...\")\n",
    "                dropped_chunk_count += 1\n",
    "            continue\n",
    "        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n",
    "\n",
    "        if extended_candidate_token_count > max_tokens:\n",
    "            output.append(chunk_delimiter.join(candidate))\n",
    "            output_indices.append(candidate_indices)\n",
    "            candidate = chunk_with_header\n",
    "            candidate_indices = [chunk_i]\n",
    "        else:\n",
    "            candidate.append(chunk)\n",
    "            candidate_indices.append(chunk_i)\n",
    "\n",
    "    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n",
    "        output.append(chunk_delimiter.join(candidate))\n",
    "        output_indices.append(candidate_indices)\n",
    "    return output, output_indices, dropped_chunk_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(\n",
    "        text: str,\n",
    "        detail: float = 0,\n",
    "        model: str = 'gpt-4-turbo',\n",
    "        additional_instructions: Optional[str] = None,\n",
    "        minimum_chunk_size: Optional[int] = 500,\n",
    "        chunk_delimiter: str = \".\",\n",
    "        summarize_recursively=False,\n",
    "        verbose=False\n",
    "        ):\n",
    "    assert 0 <= detail <= 1\n",
    "\n",
    "    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n",
    "    min_chunks = 1\n",
    "    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n",
    "\n",
    "    # 청크 크기 \n",
    "    document_length = len(tokenize(text))\n",
    "    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n",
    "    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n",
    "    if verbose:\n",
    "        print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n",
    "        print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n",
    "\n",
    "    # set system message\n",
    "    system_message_content = \"Rewrite this text in summarized form.\"\n",
    "    if additional_instructions is not None:\n",
    "        system_message_content += f\"\\n\\n{additional_instructions}\"\n",
    "\n",
    "    accumulated_summaries = []\n",
    "    for chunk in tqdm(text_chunks):\n",
    "        if summarize_recursively and accumulated_summaries:\n",
    "            # Creating a structured prompt for recursive summarization\n",
    "            accumulated_summaries_string = '\\n\\n'.join(accumulated_summaries)\n",
    "            user_message_content = f\"Previous summaries:\\n\\n{accumulated_summaries_string}\\n\\nText to summarize next:\\n\\n{chunk}\"\n",
    "        else:\n",
    "            # Directly passing the chunk for summarization without recursive context\n",
    "            user_message_content = chunk\n",
    "\n",
    "        # Constructing messages based on whether recursive summarization is applied\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "\n",
    "        # Assuming this function gets the completion and works as expected\n",
    "        response = get_chat_completion(messages, model=model)\n",
    "        accumulated_summaries.append(response)\n",
    "\n",
    "    # Compile final summary from partial summaries\n",
    "    final_summary = '\\n\\n'.join(accumulated_summaries)\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detail을 0에서 1로 증가시키면 기본 문서에 대한 요약이 점차길어진다.\n",
    "- detail 매개변수의 값이 높을수록 문서를 더 많은 수의 청크로 분할하기 때문에 더 자세한 요약이 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 1 chunks to be summarized.\n",
      "Chunk lengths are [14631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.11s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_0 = summarize(artificial_intelligence_wikipedia_text, detail=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 9 chunks to be summarized.\n",
      "Chunk lengths are [1817, 1807, 1823, 1810, 1806, 1827, 1814, 1829, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:33<00:00, 10.35s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_pt25 = summarize(artificial_intelligence_wikipedia_text, detail=0.25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 17 chunks to be summarized.\n",
      "Chunk lengths are [897, 890, 914, 876, 893, 906, 893, 902, 909, 907, 905, 889, 902, 890, 901, 880, 287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [02:56<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_pt5 = summarize(artificial_intelligence_wikipedia_text, detail=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 31 chunks to be summarized.\n",
      "Chunk lengths are [492, 427, 485, 490, 496, 478, 473, 497, 496, 501, 499, 497, 493, 470, 472, 494, 489, 492, 481, 485, 471, 500, 486, 498, 478, 469, 498, 468, 493, 478, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [04:38<00:00,  8.99s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_1 = summarize(artificial_intelligence_wikipedia_text, detail=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[286, 2538, 4531, 6748]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tokenize(x)) for x in\n",
    " [summary_with_detail_0, summary_with_detail_pt25, summary_with_detail_pt5, summary_with_detail_1]\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_with_detail_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개괄식으로 내용 요약\n",
    "summary_with_additional_instructions = summarize(artificial_intelligence_wikipedia_text, detail=0.1,\n",
    "                                                 additional_instructions=\"Write in point form and focus on numerical data.\")\n",
    "print(summary_with_additional_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재귀적으로 요약 -> 내용은 정확해짐 단 자원소모가 많음\n",
    "recursive_summary = summarize(artificial_intelligence_wikipedia_text, detail=0.1, summarize_recursively=True)\n",
    "print(recursive_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
