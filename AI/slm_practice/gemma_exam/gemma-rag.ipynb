{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 실습\n",
    "- RAG with ChromaDB\n",
    "- RAG CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 19.0.0\n",
      "Uninstalling pyarrow-19.0.0:\n",
      "  Successfully uninstalled pyarrow-19.0.0\n",
      "Requirement already satisfied: datasets in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: langchain_community in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (0.3.15)\n",
      "Requirement already satisfied: filelock in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.15 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (0.3.15)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (0.3.31)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (0.1.147)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (2.7.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.15->langchain_community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.15->langchain_community) (2.10.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.31->langchain_community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: anyio in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Using cached pyarrow-19.0.0-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pyarrow\n",
    "!pip install -U datasets langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-labs-html-chunker in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (0.0.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from google-labs-html-chunker) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from google-labs-html-chunker) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from beautifulsoup4>=4.12.2->google-labs-html-chunker) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from html5lib>=1.1->google-labs-html-chunker) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from html5lib>=1.1->google-labs-html-chunker) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-labs-html-chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_labs_html_chunker.html_chunker import HtmlChunker\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "with urlopen(\n",
    "    \"https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\"\n",
    ") as f:\n",
    "    html = f.read().decode(\"utf-8\")\n",
    "\n",
    "# Chunk the file using HtmlChunker\n",
    "chunker = HtmlChunker(\n",
    "    max_words_per_aggregate_passage=200,\n",
    "    greedily_aggregate_sibling_nodes=True,\n",
    "    html_tags_to_exclude={\"noscript\", \"script\", \"style\"}, # 필요없는 요소 제거\n",
    ")\n",
    "passages = chunker.chunk(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI Toolkit\n",
      "            \n",
      "            \n",
      "            - Google Developers Blog\n",
      "Products Develop Android Chrome ChromeOS Cloud Firebase Flutter Google Assistant Google Maps Platform Google Workspace TensorFlow YouTube Grow Firebase Google Ads Google Analytics Google Play Search Web Push and Notification APIs Earn AdMob Google Ads API Google Pay Google Play Billing Interactive Media Ads Solutions Events Learn Community Groups Google Developer Groups Google Developer Student Clubs Woman Techmakers Google Developer Experts Tech Equity Collective Programs Accelerator Solution Challenge DevFest Stories All Stories Developer Program Blog Search English English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n",
      "Products More Solutions Events Learn Community More Developer Program Blog Develop Android Chrome ChromeOS Cloud Firebase Flutter Google Assistant Google Maps Platform Google Workspace TensorFlow YouTube Grow Firebase Google Ads Google Analytics Google Play Search Web Push and Notification APIs Earn AdMob Google Ads API Google Pay Google Play Billing Interactive Media Ads Groups Google Developer Groups Google Developer Student Clubs Woman Techmakers Google Developer Experts Tech Equity Collective Programs Accelerator Solution Challenge DevFest Stories All Stories English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n",
      "Gemini Introducing PaliGemma, Gemma 2, and an Upgraded Responsible AI Toolkit MAY 14, 2024 Tris Warkentin Director, Product Management Xiaohua Zhai Senior Staff Research Scientist Ludovic Peran Product Manager Share Facebook Twitter LinkedIn Mail\n",
      "At Google, we believe in the power of collaboration and open research to drive innovation, and we're grateful to see Gemma embraced by the community with millions of downloads within a few short months of its launch. This enthusiastic response has been incredibly inspiring, as developers have created a diverse range of projects like Navarasa , a multilingual variant for Indic languages, to Octopus v2 , an on-device action model, developers are showcasing the potential of Gemma to create impactful and accessible AI solutions. This spirit of exploration and creativity has also fueled our development of CodeGemma , with its powerful code completion and generation capabilities, and RecurrentGemma , offering efficient inference and research possibilities.\n",
      "Link to Youtube Video (visible only when JS is disabled)\n",
      "Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Today, we're excited to further expand the Gemma family with the introduction of PaliGemma , a powerful open vision-language model (VLM), and a sneak peek into the near future with the announcement of Gemma 2. Additionally, we're furthering our commitment to responsible AI with updates to our Responsible Generative AI Toolkit, providing developers with new and enhanced tools for evaluating model safety and filtering harmful content.\n",
      "Introducing PaliGemma: Open Vision-Language Model PaliGemma is a powerful open VLM inspired by PaLI-3 . Built on open components including the SigLIP vision model and the Gemma language model, PaliGemma is designed for class-leading fine-tune performance on a wide range of vision-language tasks. This includes image and short video captioning, visual question answering, understanding text in images, object detection, and object segmentation. We're providing both pretrained and fine-tuned checkpoints at multiple resolutions, as well as checkpoints specifically tuned to a mixture of tasks for immediate exploration. To facilitate open exploration and research, PaliGemma is available through various platforms and resources. Start exploring today with free options like Kaggle and Colab notebooks. Academic researchers seeking to push the boundaries of vision-language research can also apply for Google Cloud credits to support their work. Get started with PaliGemma today. You can find PaliGemma on GitHub, Hugging Face models , Kaggle, Vertex AI Model Garden , and ai.nvidia.com (accelerated with TensoRT-LLM) with easy integration through JAX and Hugging Face Transformers. (Keras integration coming soon) You can also interact with the model via this Hugging Face Space .\n",
      "Screenshot from the HuggingFace Space running PaliGemma\n",
      "Announcing Gemma 2: Next-Gen Performance and Efficiency We're thrilled to announce the upcoming arrival of Gemma 2, the next generation of Gemma models. Gemma 2 will be available in new sizes for a broad range of AI developer use cases and features a brand new architecture designed for breakthrough performance and efficiency, offering benefits such as: Class Leading Performance: At 27 billion parameters, Gemma 2 delivers performance comparable to Llama 3 70B at less than half the size. This breakthrough efficiency sets a new standard in the open model landscape. Reduced Deployment Costs : Gemma 2's efficient design allows it to fit on less than half the compute of comparable models. The 27B model is optimized to run on NVIDIA’s GPUs or can run efficiently on a single TPU host in Vertex AI, making deployment more accessible and cost-effective for a wider range of users.\n",
      "Versatile Tuning Toolchains: Gemma 2 will provide developers with robust tuning capabilities across a diverse ecosystem of platforms and tools. From cloud-based solutions like Google Cloud to popular community tools like Axolotl , fine-tuning Gemma 2 will be easier than ever. Plus, seamless partner integration with Hugging Face and NVIDIA TensorRT-LLM, along with our own JAX and Keras, ensures you can optimize performance and efficiently deploy across various hardware configurations.\n",
      "Gemma 2 is still pretraining. This chart shows performance from the latest Gemma 2 checkpoint along with benchmark pretraining metrics. Source: Hugging Face Open LLM Leaderboard (April 22, 2024) and Grok announcement blog\n",
      "Stay tuned for the official launch of Gemma 2 in the coming weeks! Expanding the Responsible Generative AI Toolkit For this reason we're expanding our Responsible Generative AI Toolkit to help developers conduct more robust model evaluations by releasing the LLM Comparator in open source. The LLM Comparator is a new interactive and visual tool to perform effective side-by-side evaluations to assess the quality and safety of model' responses. To see the LLM Comparator in action, explore our demo showcasing a comparison between Gemma 1.1 and Gemma 1.0.\n",
      "We hope that this tool will advance further the toolkit’s mission to help developers create AI applications that are not only innovative but also safe and responsible. As we continue to expand the Gemma family of open models, we remain dedicated to fostering a collaborative environment where cutting-edge AI technology and responsible development go hand in hand. We're excited to see what you build with these new tools and how, together, we can shape the future of AI.\n",
      "posted in: Gemini AI Announcements Explore Learn Previous Next Related Posts AI Announcements Community Google AI Developers Community Spotlight Contest Jan. 8, 2025 Matter Smart Home Mobile Web Announcements Solutions Building a better smart home Jan. 7, 2025 Gemini AI Tutorials How-To Guides Dive Deep into Gemini: Explore  Starter Apps in AI Studio Dec. 18, 2024 AI Cloud Tutorials Best Practices Vertex AI RAG Engine: A developers tool Jan. 15, 2025 Gemini AI Announcements Community Celebrating innovation: Gemini API Developer Competition Dec. 19, 2024\n",
      "Connect Blog Instagram LinkedIn Twitter YouTube Programs Women Techmakers Google Developer Groups Google Developer Experts Accelerators Google Developer Student Clubs Developer consoles Google API Console Google Cloud Platform Console Google Play Console Firebase Console Actions on Google Console Cast SDK Developer Console Chrome Web Store Dashboard\n",
      "Android Chrome Firebase Google Cloud Platform All products Manage cookies Terms Privacy English English Español (Latam) Bahasa Indonesia 日本語 한국어 Português (Brasil) 简体中文\n"
     ]
    }
   ],
   "source": [
    "for passage in passages:\n",
    "    print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (0.5.23)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (2.10.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (3.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: anyio in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (24.12.23)\n",
      "Requirement already satisfied: protobuf in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.2)\n",
      "Requirement already satisfied: sympy in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/wonik/.pyenv/versions/3.12.5/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wonik/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:10<00:00, 7.71MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "# chroma_client.delete_collection(\"cookbook_collection\")\n",
    "collection = chroma_client.create_collection(name=\"cookbook_collection\")\n",
    "collection.add(documents=passages, ids=[str(i) for i in range(len(passages))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in answering user questions. You always understand user questions well, and then provide high-quality answers based on the information provided in the context.\n",
    "\n",
    "If the provided context does not contain relevent information, just respond \"I could not find the answer based on the context you provided.\"\n",
    "\n",
    "User question: {}\n",
    "\n",
    "Context:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "user_question = \"how many parameters does Gemma 2 have?\"\n",
    "\n",
    "results = collection.query(query_texts=user_question, n_results=3)\n",
    "\n",
    "context = \"\\n\".join(\n",
    "    [f\"{i+1}. {passage}\" for i, passage in enumerate(results[\"documents\"][0])]\n",
    ")\n",
    "prompt = f\"{prompt_template.format(user_question, context)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in answering user questions. You always understand user questions well, and then provide high-quality answers based on the information provided in the context.\n",
      "\n",
      "If the provided context does not contain relevent information, just respond \"I could not find the answer based on the context you provided.\"\n",
      "\n",
      "User question: how many parameters does Gemma 2 have?\n",
      "\n",
      "Context:\n",
      "1. Gemma 2 is still pretraining. This chart shows performance from the latest Gemma 2 checkpoint along with benchmark pretraining metrics. Source: Hugging Face Open LLM Leaderboard (April 22, 2024) and Grok announcement blog\n",
      "2. Stay tuned for the official launch of Gemma 2 in the coming weeks! Expanding the Responsible Generative AI Toolkit For this reason we're expanding our Responsible Generative AI Toolkit to help developers conduct more robust model evaluations by releasing the LLM Comparator in open source. The LLM Comparator is a new interactive and visual tool to perform effective side-by-side evaluations to assess the quality and safety of model' responses. To see the LLM Comparator in action, explore our demo showcasing a comparison between Gemma 1.1 and Gemma 1.0.\n",
      "3. Announcing Gemma 2: Next-Gen Performance and Efficiency We're thrilled to announce the upcoming arrival of Gemma 2, the next generation of Gemma models. Gemma 2 will be available in new sizes for a broad range of AI developer use cases and features a brand new architecture designed for breakthrough performance and efficiency, offering benefits such as: Class Leading Performance: At 27 billion parameters, Gemma 2 delivers performance comparable to Llama 3 70B at less than half the size. This breakthrough efficiency sets a new standard in the open model landscape. Reduced Deployment Costs : Gemma 2's efficient design allows it to fit on less than half the compute of comparable models. The 27B model is optimized to run on NVIDIA’s GPUs or can run efficiently on a single TPU host in Vertex AI, making deployment more accessible and cost-effective for a wider range of users.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the answer\n",
    "- 4bit 양자화 gemma 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import bitsandbytes, accelerate\n",
    "\n",
    "model = \"google/gemma-1.1-7b-it\"\n",
    "#mdel = \"google/gemma_instruct_2b_en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
    "print(outputs[0][\"generated_text\"][len(prompt) :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG CSV 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 드라이브 마운트 (필요 없을 경우 skip)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#get movies dataset\n",
    "dataset = load_dataset(\"MongoDB/embedded_movies\")\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/example_pdfs/train_0.csv')\n",
    "df\n",
    "dataset_df = df #pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.dropna(subset=[\"original\"])\n",
    "# 필요한 정보들만 추출\n",
    "main_dataset_df = dataset_df[[\"original\", \"modern translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "#convert DataFrane into Langchain Document format for further processing\n",
    "#\"fullplot\" will be the main content information, \"title\" and \"generes\" will be used as metadata\n",
    "loader = DataFrameLoader(main_dataset_df, page_content_column=\"original\")\n",
    "dataset_docs = loader.load()\n",
    "\n",
    "dataset_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking using embedding model\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 문장단위로 나누기 위한 구분자들\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\*\\*\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "#\"thenlper/gte-small\" with 512 dimentional embedding is used as embedding model\n",
    "EMB_MODEL_CKP = \"thenlper/gte-small\" # 임베딩 모델\n",
    "#get enbedding_tokenizer\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_CKP)\n",
    "\n",
    "def split_documents(chunk_size, KB, tokenizer=embedding_tokenizer):\n",
    "  \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "  \"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        embedding_tokenizer, #tokenizer to be used to determine number of tokens\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True, # If `True`, includes chunk's start index in metadata\n",
    "        strip_whitespace=True, # If `True`, strips whitespace from the start and end of every document\n",
    "        separators=MARKDOWN_SEPARATORS, #use seperators for chunking\n",
    "    )\n",
    "\n",
    "  docs_processed = []\n",
    "  for doc in KB:\n",
    "      docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "  #중복된 문장 제거\n",
    "  unique_texts = {}\n",
    "  docs_processed_unique = []\n",
    "  for doc in tqdm(docs_processed):\n",
    "    if doc.page_content not in unique_texts:\n",
    "      unique_texts[doc.page_content] = True\n",
    "      docs_processed_unique.append(doc)\n",
    "\n",
    "  return docs_processed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split documents\n",
    "docs_processed_tok = split_documents(512, dataset_docs, EMB_MODEL_CKP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(embedding_tokenizer.encode(doc.page_content)) for doc in tqdm(dataset_docs)]\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens) before\")\n",
    "plt.show()\n",
    "\n",
    "lengths = [len(embedding_tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed_tok)]\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens) after chunking\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = EMB_MODEL_CKP,\n",
    "    multi_process = True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")\n",
    "\n",
    "#create FAISS indices for approximate nearest neighbour search\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    dataset_docs, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_search_result(query, vector_db):\n",
    "  \"\"\"\n",
    "  given a \"query\" search for top k \"original\" embedded in vector database\n",
    "  \"\"\"\n",
    "  #get top k documents similar to \"query\"\n",
    "  retrieved_docs = vector_db.similarity_search(query=user_query, k=3)\n",
    "  search_result = \"\"\n",
    "  for result in retrieved_docs:\n",
    "      retrieved_plot = result.page_content if result.page_content else \"N/A\"\n",
    "\n",
    "      retrieved_translation = result.metadata['modern translation'] if result.metadata['modern translation'] else \"N/A\"\n",
    "      search_result += f\"original: {result.page_content}, Modern Translation: {retrieved_translation}\"\n",
    "\n",
    "  return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct query with retrieval of sources\n",
    "#user_query = \"What is a good romance movie to watch and why?\"\n",
    "user_query = \"\"\n",
    "retrieved_results = get_search_result(user_query, KNOWLEDGE_VECTOR_DATABASE)\n",
    "combined_information = f\"Please answer the following query using the context provided. Please find the translation. \\n :\\n{retrieved_results}. \\n : {user_query}\"\n",
    "#chat template for gemma model conversation\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": combined_information },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to GPU\n",
    "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "pipe = pipeline(model=model,\n",
    "                tokenizer = tokenizer,\n",
    "                task=\"text-generation\",\n",
    "                return_full_text=False,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=True,\n",
    "                temperature=2.1,\n",
    "                top_k=50,\n",
    "                top_p=1,\n",
    "                #repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                #add_special_tokens=True,\n",
    "                )\n",
    "print(f\"Query: {user_query}\\n\")\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct query with retrieval of sources\n",
    "user_query = \"'신즁의부인의도도ᄒᆞᆫ고집을ᄋᆡ달나무슈히ᄎᆞ탄ᄒᆞ시고외당으로나오시니마ᄎᆞᆷ시비춘셤이상을드리거날좌우고요' 해석해줘\"\n",
    "retrieved_results = get_search_result(user_query, KNOWLEDGE_VECTOR_DATABASE)\n",
    "combined_information = f\"Please answer the following query using the context provided. Please find the translation. \\n :\\n{retrieved_results}. \\n : {user_query}\"\n",
    "#chat template for gemma model conversation\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": combined_information },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to GPU\n",
    "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "pipe = pipeline(model=model,\n",
    "                tokenizer = tokenizer,\n",
    "                task=\"text-generation\",\n",
    "                return_full_text=False,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=True,\n",
    "                temperature=2.1,\n",
    "                #top_k=50,\n",
    "                #top_p=1,\n",
    "                #repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                #add_special_tokens=True,\n",
    "                )\n",
    "print(f\"Query: {user_query}\\n\")\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
