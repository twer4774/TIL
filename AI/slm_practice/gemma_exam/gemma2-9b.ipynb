{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-9b\n",
    "- GPU 큰것 필요 ex. A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('user_name')\n",
    "os.envrion[\"KAGGLE_Key\"] = userdata.get('kaggle')\n",
    "\n",
    "os.envrion[\"KERAS_BACKEND\"] = \"jax\" # Or \"tensorflow\" or \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U keras-nlp\n",
    "!pip install -U keras==3.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import keras\n",
    "\n",
    "# 연산수행\n",
    "keras.config.set_floatx(\"bfloat16\")\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_9b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gemma_lm.generate(\"It was a dark and stormy night.\", max_length=256)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata, drive\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"user_name\")\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"kaggle\")\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U keras-nlp datasets\n",
    "!pip install -q -U keras\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" Or \"torch\" or \"tensorflow\".\n",
    "\n",
    "# 트레이닝 설정\n",
    "lora_name = \"translator\"\n",
    "lora_rank = 8\n",
    "lr_value = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import torch\n",
    "\n",
    "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma_instruct_2b_en\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 한글 데이터셋\n",
    "ds = load_dataset(\"nlpai-lab/kullm-v2\", split=\"train\")\n",
    "\n",
    "print(ds)\n",
    "data = ds.with_format(\n",
    "    \"np\", columns=[\"instruction\", \"output\"], output_all_columns=False\n",
    ")\n",
    "train = []\n",
    "max_token_len = 0\n",
    "\n",
    "for x in data:\n",
    "    item = f\"user\\n{x['instruction']}\\nmodel\\n{x['output']}\"\n",
    "    train.append(item)\n",
    "    length = tokenizer(item).get_shape().as_list()[0]\n",
    "    if length > max_token_len:\n",
    "        max_token_len = length\n",
    "        print(f\"longest toekn {max_token_len} - {item}\")\n",
    "\n",
    "print(train[0])\n",
    "print(train[1])\n",
    "print(train[2])\n",
    "\n",
    "print(max_token_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "import time\n",
    "\n",
    "gemma = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n",
    "gemma.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import time\n",
    "\n",
    "gemma = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n",
    "gemma.summary()\n",
    "\n",
    "tick_start = 0\n",
    "\n",
    "# 가지고 오는데 얼마나 걸리는지\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "\n",
    "def text_gen(prompt):\n",
    "    tick()\n",
    "    input = f\"user\\n{prompt}\\nmodel\\n\"\n",
    "    output = gemma.generate(input, max_length=512)\n",
    "    print(\"\\nGemma output:\")\n",
    "    print(output)\n",
    "    tock()\n",
    "\n",
    "\n",
    "text_gen(\"비언어적 커뮤니케이션에 대해 알려주세요\")\n",
    "#비언어적 커뮤니케이션도 언어적 커뮤니케이션만큼이나 중요합니다. 몸짓, 표정, 목소리 톤은 모두 메시지가 인식되는 방식에 중요한 역할을 합니다. 비언어적 단서가 언어적 메시지와 일치하는지 확인하세요. 예를 들어, 눈을 마주치고 열린 자세를 유지하는 것은 자신감과 진정성을 전달할 수 있습니다. 마찬가지로, 다른 사람의 비언어적 단서에 유의하세요. 비언어적 단서는 화자가 진정으로 무엇을 느끼고 생각하는지에 대한 귀중한 통찰력을 제공할 수 있습니다.\n",
    "\n",
    "text_gen(\"존슨앤드존슨 COVID-19 백신\")\n",
    "#2020년 12월 11일 - 미국에서 화이자/바이오엔텍 COVID-19 백신이 긴급 사용 승인을 받았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank (4, 8 or 16).\n",
    "gemma.backbone.enable_lora(rank=lora_rank)\n",
    "gemma.summary()\n",
    "\n",
    "# Limit the input sequence length to 128 (to control memory usage).\n",
    "gemma.preprocessor.sequence_length = 2048\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=lr_value,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA epoch 저장\n",
    "- 시간이 너무 오래 걸림. # 1266:40:17 분 걸림\n",
    "- 아래 대체방법으로 Huggingface이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 실습시 삭제\n",
    "# train_tmp = train[:5]\n",
    "\n",
    "# Reduce the sequence length to a more manageable size.\n",
    "gemma.preprocessor.sequence_length = 512  # Or an even smaller value if needed\n",
    "\n",
    "for x in range(2):\n",
    "    gemma.fit(train_tmp, epochs=1, batch_size=1)\n",
    "    model_name = f\"/content/drive/MyDrive/gemma_lora/{lora_name}_{lora_rank}_epoch{x+1}.lora.h5\"\n",
    "    gemma.backbone.save_lora_weights(model_name)\n",
    "\n",
    "    text_gen(\"비언어적 커뮤니케이션에 대해 알려주세요\")\n",
    "    #비언어적 커뮤니케이션도 언어적 커뮤니케이션만큼이나 중요합니다. 몸짓, 표정, 목소리 톤은 모두 메시지가 인식되는 방식에 중요한 역할을 합니다. 비언어적 단서가 언어적 메시지와 일치하는지 확인하세요. 예를 들어, 눈을 마주치고 열린 자세를 유지하는 것은 자신감과 진정성을 전달할 수 있습니다. 마찬가지로, 다른 사람의 비언어적 단서에 유의하세요. 비언어적 단서는 화자가 진정으로 무엇을 느끼고 생각하는지에 대한 귀중한 통찰력을 제공할 수 있습니다.\n",
    "\n",
    "    text_gen(\"존슨앤드존슨 COVID-19 백신\")\n",
    "    #2020년 12월 11일 - 미국에서 화이자/바이오엔텍 COVID-19 백신이 긴급 사용 승인을 받았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위의 방법 대체 - 시간 단축\n",
    "- SFTTrainer 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "login(\n",
    "    token=userdata.get('huggingface'),\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 준비\n",
    "## 네이버 기사 요약 데이터셋\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "dataset = load_dataset(\"wonik-hi/korea_summary_Thesis\")\n",
    "\n",
    "## dataset은 train과 train, validation, test로 구분되어 있음\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma 모델의 한국어 요약 테스트\n",
    "\n",
    "### 모델로드\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)\n",
    "doc = dataset['train']['instruction'][0]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"다음 내용을 자연스럽고 문맥에 맞게 완성하세요:\\n\\n{}\".format(doc)\n",
    "     }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(outputs[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-it 파인튜닝\n",
    "def generate_prompt(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['insturction'])):\n",
    "        messages = [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "             \"content\": \"다음 내용을 자연스럽고 문맥에 맞게 완성하세요:\\n\\n {}\".format(example['insturction'][i])\n",
    "             },\n",
    "             {\n",
    "                 \"role\": \"assistant\",\n",
    "                 \"content\": \"{}\".format(example['output'][i])\n",
    "                 }\n",
    "        ]\n",
    "        chat_message = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(chat_message)\n",
    "\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
    "                           다은 매용을 자연스럽고 문맥에 맞게 완성하고, 글을 요약해주세요:\n",
    "                           \n",
    "                           {}<end_of_turn>\n",
    "                           <start_of_turn>model\n",
    "                           {}<end_of_turn><eos>\n",
    "                           \"\"\".format(example['instruction'][i], example['output'][i]))\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "print(generate_prompt(train_data[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL=\"google/gemma-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer 실행\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        max_steps=1000,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_steps=0.03,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "    ),\n",
    "    peft_cofnig=lora_config,\n",
    "    formatting_func=generate_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter\"\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('gemma-2b-it-ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 요약 모델 추론\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "FINETUNE_MODEL = \"./gemma-2b-it-ko\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokneizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"다음 내용을 자연스럽고 문맥에 맞게 완성하고, 글을 요약해주세요.\\n\\n{}\".format(doc)\n",
    "    }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe_finetuned(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "print(outputs[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"wonik-hi/gemma-2b-it-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UFT-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "\n",
    "MODEL = \"./gemma-2b-it-ko\"\n",
    "\n",
    "st.header(\"Streamlit Application\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(MODEL, revision=\"fp16\", torch_dtype=torch.float16, use_auto_token=True)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "with st.form('form', clear_on_submit = True):\n",
    "    prompt = st.text_input(\"Prompt: \", \"\")\n",
    "    submitted = st.form_submit_button('Generate')\n",
    "\n",
    "if submitted and prompt:\n",
    "    prompt = [prompt]\n",
    "    \n",
    "    print(outputs[0][\"generated_text\"][len(prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run /content/app.py &>/content/logs.txt &\n",
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
