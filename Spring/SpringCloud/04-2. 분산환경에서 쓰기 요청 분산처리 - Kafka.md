# 04-2. 분산환경에서 쓰기 요청 분산처리 - Kafka

## Kafka

- https://kafka.apache.org
- TCP/IP 방식으로 파일로 저장
- Spring for Apache Kafka이용
- kafka는 내부적으로 zookeeper와 연동되어 있다.

- 여러 대의 분산 서버에서 대량의 데이터를 처리할 수 있다.
- 애플, 드롭박스, 넷플릭스, 트위터, 우버 등에서 카프카를 사용하고 있다.
- 참고 : https://baek.dev/post/20/

### 특징

- 높은 처리량으로 실시간 처리
- 임의의 타이밍에서 데이터를 읽을 수 있다.
- 다양한 제품과 연동
- 메시지를 잃어버리지 않는다.
  - Ack : 브로커가 메시지를 수신했을 때 프로듀서에게 수신 완료했다는 응답을 보낸다.
    - 설정
    - 0 : 프로듀서는 메시지 송신 시 Ack를 기다리지 않고 다음 메시지를 전송한다.
    - 1 : Leader 레플리카에 메시지가 전달되면 Ack를 반환한다.
    - All : 모든 ISR(In-Sync Replica, 복제 상태의 레플리카)의 수만큼 복제되면  Ack를 반환한다.
  - Offset Commit : 컨슈머가 브로커로부터 메시지를 받을 때 컨슈머가 어디까지 메시지를 받았는지 관리한다.

### 구성요소

- Brocker : 데이터를 수신, 전달하는 서비스
  - 하나의 서버 당 하나의 데몬 프로세스로 동작한다.
  - 여러 대의 클러스터로 구성할 수 있다. => 스케일 아웃이 가능하다.
- Message : 카프카에서 다루는 데이터의 최소 단위
- Producer : 데이터 생산자
- Consumer : 브로커에서 메시지를 취득하는 어플리케이션
  - Consumer Group : group.id로 지정한다. 클러스터의 메시지를 얻을 때 그룹단위로 얻는다.
- Topic : 메시지를 종류별로 관리하는 스토리지
  - Partition : 토픽에 대한 대량의 메시지 입출력 지원을 위해 브로커 상의 데이터를 읽고 쓰는 것을 파티션이라는 단위로 분할한다.(배치의 Chunk 같은 느낌)
    - 적정 파티션 수 : 한 번 증가시킨 파티션은 다시 줄일 수 없으므로 설계시 고려필요
  - Offset : 각 파티션에서 수신한 메시지에 일련번호를 부여한다.
    - Log-End-Offset(LEO) : 파티션 데이터의 끝
    - Current Offset : 컨슈머가 어디까지 메시지를 읽었는가를 나타냄
    - Commit Offset : 컨슈머가 어디까지 커밋했는지를 나타냄

### 카프카의 레플리카

- 카프카는 메시즈를 중계함과 동시에 수신한 메시지를 잃지 않기 위해 레플리카 구조를 지닌다.
  - Leader : Follower에게 메시지 공급. 프로듀서, 컨슈머와의 데이터 교환 담당
  - Follower : Leader의 메시지를 복제하여 유지

## 실습

```groovy
dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation 'org.springframework.kafka:spring-kafka:2.6.7'

    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.0'
}

dependencyManagement {
    imports {
        mavenBom("org.springframework.cloud:spring-cloud-dependencies:2020.0.1")
    }
}
```

- application.yml

```yml
#producer etc default
spring.kafka:
  bootstrap-servers: 127.0.0.1:9092

#consumer
spring.kafka.consumer:
  bootstrap-servers: 127.0.0.1:9092
  group-id: thecodinglive
  enable-auto-commit: true
  auto-commit-interval: 1000ms
  auto-offset-reset: latest
```

- docker-compose.yml
  - kafka 실행 -> docker-compose up

```yml
version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    ports:
      - "12181:2181"
  kafka:
    image: wurstmeister/kafka:2.12-2.5.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
      KAFKA_CREATE_TOPICS: "walter"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
```

- util

```java
package com.walter.kafka.util;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;


public class CustomJacksonConverter {

    private static final ObjectMapper mapper;

    static {
        mapper = new ObjectMapper()
                .setSerializationInclusion(JsonInclude.Include.NON_NULL)
                .configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false)
                .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)
                .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false)
                .configure(DeserializationFeature.ADJUST_DATES_TO_CONTEXT_TIME_ZONE, false)
                .registerModule(new JavaTimeModule());
    }


    private CustomJacksonConverter(){
        throw new RuntimeException("construct not support");
    }

    public static ObjectMapper getInstance(){
        return mapper;
    }

    public static String toJson(Object value){
        try{
            return mapper.writeValueAsString(value);
        }catch(Exception e){
            throw new RuntimeException("convert error");
        }
    }

    public static <T> T toObject(String content, Class<T> valueType){
        try{
            return mapper.readValue(content, valueType);
        } catch(Exception e){
            throw new RuntimeException("convert error");
        }
    }

    public static <T> T toObject(String content, TypeReference<T> type) {
        try {
            return mapper.readValue(content, type);
        } catch (Exception e) {
            throw new RuntimeException("convert error");
        }
    }

    public static <T> T convert(Object obj, Class<T> type) {
        try {
            return mapper.convertValue(obj, type);
        } catch (Exception e) {
            throw new RuntimeException("convert error");
        }
    }
}

```

- MyEvent

```java
package com.walter.kafka.eventService;

import java.util.Map;
import java.util.UUID;

public class MyEvent {

    private String eventId;
    private Map<String, Object> myData;

    public MyEvent(Map<String, Object> myData){
        this.eventId = UUID.randomUUID().toString();
        this.myData = myData;
    }

    public String getEventId() {
        return eventId;
    }

    public Map<String, Object> getMyData() {
        return myData;
    }

    public void setEventId(String eventId) {
        this.eventId = eventId;
    }

    public void setMyData(Map<String, Object> myData) {
        this.myData = myData;
    }

    @Override
    public String toString() {
        return "MyEvent{" +
                "eventId='" + eventId + '\'' +
                ", myData=" + myData +
                '}';
    }
}

```

- Controller

```java
package com.walter.kafka.eventService;

import lombok.RequiredArgsConstructor;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RequiredArgsConstructor
@RestController
public class EventController {

    private final MyEventService myEventService;

    @GetMapping("/event")
    public String getEvent(){
        myEventService.sendMsg();
        return "OK";
    }

}

```

- Service

```java
package com.walter.kafka.eventService;

import com.walter.kafka.sender.KafkaProducer;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;

import java.util.HashMap;
import java.util.Map;

@RequiredArgsConstructor
@Service
public class MyEventService {

    private final KafkaProducer kafkaProducer;

    public void sendMsg(){
        Map<String, Object> data = new HashMap<>();
        data.put("width", 1020);
        data.put("height", 7090);

        kafkaProducer.send(KafkaProducer.TOPIC_NAME, new MyEvent(data));
    }


}

```

- Sender

```java
package com.walter.kafka.sender;

import com.walter.kafka.util.CustomJacksonConverter;
import lombok.RequiredArgsConstructor;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;

@RequiredArgsConstructor
@Component
public class KafkaProducer {

    public static final String TOPIC_NAME = "walter";

    private final KafkaTemplate<String, String> kafkaTemplate;


    public void send(String topic, Object data){
        try{
            kafkaTemplate.send(topic, CustomJacksonConverter.toJson(data));
        }catch(Exception e){
            System.err.println("error"  + e.getMessage());
        }
    }

}

```

- Receiver

```java
package com.walter.kafka.receiver;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

import static com.walter.kafka.sender.KafkaProducer.TOPIC_NAME;

@Component
public class KafkaReceiver {

    @KafkaListener(topics = TOPIC_NAME, autoStartup = "true")
    public void eventHandler(Object event){
        System.out.println("get data: " + event);
    }
}
```

- localhost:8080/event 접속 시 터미널에 관련 메시지 문구 출력

